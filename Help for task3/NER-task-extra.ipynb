{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сопроводительный ноутбук к заданию №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Оглавление** <a id='toc'></a>\n",
    "\n",
    "1. [Рекуррентные нейронные сети](#section_rnn)\n",
    "  1. [Векторные представления слов](#ssection_embeddings)\n",
    "  2. [Функционирование RNN](#ssection_rnn_functioning)\n",
    "  3. [Внутреннее устройство RNN ячеек](#ssection_rnn_cells)\n",
    "    1. [Ячейка \"RNN\" (`torch.nn.RNNCell`)](#sssection_rnn_cell)\n",
    "    2. [Ячейка \"Управляемый рекуррентный блок\" (GRU, `torch.nn.GRUCell`)](#sssection_gru_cell)\n",
    "    3. [Ячейка \"Долгая краткосрочная память\" (LSTM, `torch.nn.LSTMCell`)](#sssection_lstm_cell)\n",
    "  4. [RNN-ячейки в `torch`](#ssection_torch_rnn_cells)\n",
    "  5. [RNN слои в `torch`](#ssection_torch_rnn)\n",
    "  6. [Пример использования RNN слоев в `torch`](#ssection_torch_rnn_example)\n",
    "  7. [Упаковка последовательностей разной длины](#ssection_sequences_packing)\n",
    "    1. [Упаковка последовательности](#sssection_sequences_packing_filtration)\n",
    "        1. [Несколько трюков](#ssssection_sequences_packing_filtration_tricks)\n",
    "    2. [Уравнивание длин последовательностей (метод набивки)](#sssection_sequences_packing_padding)\n",
    "    3. [Реализация подходов упаковки и набивки в `torch`](#sssection_sequences_packing_torch)\n",
    "        1. [\"Чистый\" метод упаковки](#ssssection_sequences_packing_torch_packing)\n",
    "        2. [\"Чистый\" метод набивки](#ssssection_sequences_packing_torch_padding)\n",
    "        3. [Совмещение обоих подходов](#ssssection_sequences_packing_torch_mixed)\n",
    "  8. [Специальные токены](#ssection_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рекуррентные нейронные сети (RNN) <a id='section_rnn'></a>\n",
    "**Рекуррентные нейронные сети** (**RNN**) — вид нейронных сетей, предназначенных для обработки последовательностей произвольной длины, поэтому с помощью них можно решать задачи, связанные с текстами и временными рядами. В рамках домашнего задания вам будет предложено поработать с текстами.\n",
    "\n",
    "[К оглавлению](#toc)\n",
    "\n",
    "## Векторные представления слов <a id='ssection_embeddings'></a>\n",
    "Для того, чтобы подать последовательность слов $t_1, t_2, \\dots, t_T$ на вход RNN, нужно сначала каждое слово перевести в его **эмбеддинг** или, по-русски, в **векторное представление**.\n",
    "Для этого сначала составляют множество слов (**словарь**) $W$, для которых будут обучаться эмбеддинги. Все слова во входной последовательности, не попавшие в словарь, будут обработаны особым способом. Далее каждому слову $w \\in W$ сопоставляется уникальный индекс от $0$ до $|W| - 1$ включительно. После этого используют дополнительный слой `torch.nn.Embedding`, который как раз и делает основную работу — переводит индекс слова из словаря в соответствующий ему обучаемый эмбеддинг. Размерность эмбеддингов будем обозначать за $d_{in}$. Обычное значения для $d_{in}$ — порядка сотен: 50, 100, 200, 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример, в котором есть два входных текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    [\"you\", \"are\", \"my\", \"friend\"],\n",
    "    [\"dear\", \"friend\", \"where\", \"are\", \"you\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим руками словарь из всех слов из примера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"you\",\n",
    "    \"are\",\n",
    "    \"my\",\n",
    "    \"friend\",\n",
    "    \"dear\",\n",
    "    \"where\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим руками отображение слов из словаря в индексы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind = {\n",
    "    \"you\": 0,\n",
    "    \"are\": 1,\n",
    "    \"my\": 2,\n",
    "    \"friend\": 3,\n",
    "    \"dear\": 4,\n",
    "    \"where\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем руками слова в текстах в соответствующие индексы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 3, 5, 1, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим слой `torch.nn.Embedding` для перевода индексов в векторные представления (для примера положим $d_{in}$ равным 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch          # Импортируем библиотеку torch,\n",
    "from torch import nn  #  а также модуль nn для работы с нейронными сетями\n",
    "\n",
    "d_in = 2\n",
    "\n",
    "torch.manual_seed(42)  # Для воспроизводимости инициализации эмбеддингов\n",
    "embeddings = nn.Embedding(num_embeddings=len(dictionary), embedding_dim=d_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на то, как теперь выглядит каждый из текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово   -> Индекс -> Эмбеддинг                 \n",
      "-----------------------------------------------\n",
      "you     ->      0 -> tensor([0.3367, 0.1288])\n",
      "are     ->      1 -> tensor([0.2345, 0.2303])\n",
      "my      ->      2 -> tensor([-1.1229, -0.1863])\n",
      "friend  ->      3 -> tensor([ 2.2082, -0.6380])\n",
      "-----------------------------------------------\n",
      "dear    ->      4 -> tensor([0.4617, 0.2674])\n",
      "friend  ->      3 -> tensor([ 2.2082, -0.6380])\n",
      "where   ->      5 -> tensor([0.5349, 0.8094])\n",
      "are     ->      1 -> tensor([0.2345, 0.2303])\n",
      "you     ->      0 -> tensor([0.3367, 0.1288])\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "header = f\"{'Слово':7} -> {'Индекс':6} -> {'Эмбеддинг':26}\"\n",
    "print(header)\n",
    "print('-' * len(header))\n",
    "\n",
    "for text, text_idxs in zip(texts, idxs):\n",
    "    # Получаем эмбеддинги по индексам слов\n",
    "    embeds = embeddings(torch.tensor(text_idxs))\n",
    "\n",
    "    # Печатаем слова, индексы и эмбеддинги\n",
    "    for word, idx, embedding in zip(text, text_idxs, embeds):\n",
    "        print(f\"{word:7} -> {idx:6} -> {embedding.data}\")\n",
    "    print('-' * len(header))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить, одному и тому же слову в разных предложениях соответствует один и тот же эмбеддинг.\n",
    "\n",
    "Мы не обучаем векторные представления сразу для всех слов из датасета, потому что уникальных слов может быть очень много (сотни тысяч), из-за чего словарь будет большим и, как следствие, слой `torch.nn.Embedding` будет занимать много памяти.\n",
    "Например, для словаря из $500 000$ слов и размерности векторных представлений $300$ понадобится порядка $(500000 \\cdot 300 \\cdot 8)/2^{30} \\approx 1.12$ Гб памяти. На современных пользовательских GPU обычно имеется порядка 2-11 Гб видео памяти, поэтому каждый Гб, занимаемый вашим приложением, будет на счету.\n",
    "\n",
    "Также обучать эмбеддинги для редких слов, которых обычно много, практически бессмысленно, потому что из-за нехватки обучающих данных векторные представления для них будут малоинформативными.\n",
    "\n",
    "К тому же во время инференса модели (то есть использования модели уже после обучения для получения предсказаний) все равно могут попасться на вход какие-нибудь неизвестные ей слова. Тут уже не получится справиться с этой проблемой расширением словаря, потому что у нас нет обучающих данных с этими неизвестными словами.\n",
    "\n",
    "Также можно использовать в качестве эмбеддингов предобученные представления слов, например, [word2vec](https://huggingface.co/fse/word2vec-google-news-300), [Glove](https://nlp.stanford.edu/projects/glove/), [FastText](https://fasttext.cc/docs/en/crawl-vectors.html).\n",
    "\n",
    "Со словами, не попавшими в словарь, можно поступить несколькими способами:\n",
    "1. Удалить все неизвестные слова;\n",
    "2. Заменить каждое такое слово на специальное слово \"`<UNK>`\";\n",
    "3. Составлять словарь не из слов, а из небольших частей слов.\n",
    "\n",
    "Если с первым пунктом все понятно, то оставшиеся требуют пояснений.  \n",
    "Во втором способе предлагается в словарь добавить специальное слово \"`<UNK>`\" и для него обучать векторное представление. Во входной последовательности $t_1, \\dots, t_T$ до перевода ее в последовательность эмбеддингов все неизвестные слова заменяются на слово \"`<UNK>`\", после чего перевод слов в эмбеддинги продолжается обычным способом.\n",
    "В специальном слове \"`<UNK>`\" используются треугольные скобки для того, чтобы не путать это слово с неспециальными словами.  \n",
    "Про последний способ подробно говорить в этом материале не будем.\n",
    "\n",
    "Далее будем считать, что входную последовательность слов $t_1, t_2, \\dots, t_T$ мы уже перевели в последовательность соответствующих им эмбеддингов $x_1, x_2, \\dots, x_T$.\n",
    "\n",
    "**Резюме главы**\n",
    "1. RNN позволяют решать задачи, в которых длина входа произвольна.\n",
    "2. Входные слова перед подачей в RNN надо превратить в эмбеддинги. Это можно сделать с помощью `torch.nn.Embedding` слоя.\n",
    "3. Нужно как-то бороться с проблемой неизвестных слов.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функционирование RNN <a id='ssection_rnn_functioning'></a>\n",
    "Высокоуровнево функционирование одного слоя RNN выглядит следующим образом:\n",
    "\n",
    "![Unrolled RNN](images/rnn-unrolled.png \"Unrolled RNN\")\n",
    "\n",
    "RNN слой на рисунке обозначен буквой А. В этом слое есть какие-то обучающиеся параметры, благодаря которым RNN как раз и обучается. В нашем случае RNN слой принимает на вход последовательность $x_1, x_2, \\dots, x_T \\in \\mathbb{R}^{d_{in}}$ (на рисунке нумерация индексов начинается с 0, но мы в тексте будем придерживаться нумерации с 1). Справа на рисунке изображен один и тот же слой А для того, чтобы продемонстрировать, как он функционирует.\n",
    "\n",
    "Для обработки каждой отдельной входной последовательности $x_1, x_2, \\dots, x_T \\in \\mathbb{R}^{d_{in}}$ в RNN заводится внутреннее состояние $h \\in \\mathbb{R}^{d_{hidden}}$, которое обычно инициализируется случайно или нулями.\n",
    "\n",
    "Далее через RNN начинают пропускать входные эмбеддинги по одному. На шаге $t = \\overline{1, T}$ эмбеддинг $x_t$ подается в RNN. RNN внутри себя пережевывает $x_t$ и $h$, после чего обновляет внутреннее состояние $h$ и выдает выходной эмбеддинг $y_t \\in \\mathbb{R}^{d_{hidden}}$. Состояние сети после шага $t$ будем обозначать за $h_t$.\n",
    "\n",
    "На рисунке изображено, что сеть в качестве $y_t$ выдает внутреннее состояния без изменения. \n",
    "Обычно так и делают, хотя ничего не мешает как-нибудь преобразовать $h_t$ перед выдачей из сети.\n",
    "\n",
    "После того, как вся последовательность $x_1, \\dots, x_T$ была пропущена через RNN, имеем на выходе из сети последовательность $y_1, \\dots, y_T \\in \\mathbb{R}^{d_{hidden}}$.\n",
    "\n",
    "Уже такая модель (пока что мы не специфицировали, что модель делает конкретно внутри себя, но это мы опишем чуть позже) может решать задачи машинного обучения. Как именно — зависит от самой задачи. Например, для классификации входного предложения $t_1, \\dots, t_T$ целиком как единой сущности можно обучать MLP-классификатор поверх последнего выходного вектора $y_T$, что логично, так как в $y_T$ находится информация о всем входном тексте. Для разметки каждого слова последовательности, например, в задачах определения частей речи (PoS) или выделения именованных сущностей (NER), можно обучить MLP-классификатор на каждом выходном векторе $y_t$.\n",
    "\n",
    "Слой RNN, который мы описали выше, обычно называют не слоем, а **ячейкой** (**cell**, **RNN cell**). Будем и дальше придерживаться такой терминологии, а под слоем будет понимать более сложную вещь, речь о которой пойдет дальше.\n",
    "\n",
    "Для некоторых задач такая простая модель может показывать не очень высокое качество, поэтому есть несколько способов ее усложнить.\n",
    "\n",
    "Как можно понять, выходной вектор $y_t$ зависит только от первых $t$ слов предложения: от $t_1, t_2, \\dots, t_t$. В случае решения задачи NER такого **левостороннего контекста** может оказаться недостаточно для хорошего качества модели. Для того, чтобы в $y_t$ учесть еще и **правосторонний контекст**, обычно добавляют слой RNN, описанный выше, только в него вход $x_1, \\dots, x_T$ подается задом наперед: сначала подается $x_T$, затем $x_{T-1}, x_{T-2}, \\dots, x_2$ и только в самом конце $x_1$. Обозначим выход изначальной RNN, в которую вход подавался слева направо, за $\\overrightarrow{y_1}, \\overrightarrow{y_2}, \\dots, \\overrightarrow{y_T}$, а выход второй RNN за $\\overleftarrow{y_1}, \\overleftarrow{y_2}, \\dots, \\overleftarrow{y_T}$. Тогда выход $y_t \\in \\mathbb{R}^{2 \\cdot d_{hidden}}$ этих двух слоев получим с помощью конкатенации векторов $\\overrightarrow{y_t}$ и $\\overleftarrow{y_t}$.\n",
    "Теперь в $y_t$ учитывается двусторонний контекст, а такая модель должна уже лучше справится с задачей NER. Такую RNN называют **двунаправленной** (**bidirectional**).\n",
    "\n",
    "![BiRNN layer](images/birnn-layer.png \"BiRNN layer\")\n",
    "На рисунке в качестве А обозначена RNN, читающая вход слева-направо, а за А' — RNN, читающая вход справа-налево. Скрытые состояния обозначены буквами $s$ и $s'$.\n",
    "\n",
    "Можно еще пуще усложнить полученную модель. Как видим, RNN на вход приняла последовательность векторов ($x_1, \\dots, x_T$) и выдала на выход последовательность векторов ($y_1, \\dots, y_T$). Тогда можно создать еще один такой же слой RNN и пропустить через него $y_1, \\dots, y_T$. Новый слой RNN будет отличаться единственным изменением размерности входных векторов — первый слой RNN принимал на вход вектора размерности $d_{in}$, а сейчас мы хотим подавать на вход вектора размерности $d_{hidden}$ или $2 \\cdot d_{hidden}$ в зависимости от направленности сети. Пусть выходом нового слоя станет последовательность $z_1, \\dots, z_T \\in \\mathbb{R}^{d_{hidden}}$ (или $z_1, \\dots, z_T \\in \\mathbb{R}^{2 \\cdot d_{hidden}}$ в случае двунаправленного слоя). Мы с ней дальше можем делать все то же самое, что могли делать с последовательностью $y_1, \\dots, y_T$, в том числе и решать вышеописанные задачи.\n",
    "\n",
    "Таким образом можно составить RNN из произвольно большого числа слоев. Однако, на практике редко используют больше 2-3 слоев из-за проблем обучения таких глубоких сетей.\n",
    "\n",
    "![Multi-layer RNN](images/multilayer-rnn.svg \"Multi-layer RNN\")\n",
    "Рисунок RNN с несколькими слоями. Тут за $H_t^{(l)}$ обозначен выход (и внутреннее состояние) RNN слоя с номером $l$ после обработки $t$-го элемента входной последовательности.\n",
    "\n",
    "Стоит добавить, что в каждом отдельном слое RNN поддерживается свое независимое внутреннее состояние $h$. Отдельными слоями RNN являются слои разных направлений (слева-направо и справа-налево), а также слои, находящиеся на разных уровнях (например, слой, превращающий $x$ в $y$ и слой, превращающий $y$ в $z$, находятся на разных уровнях).\n",
    "\n",
    "Кстати, двунаправленные RNN иногда нет возможности использовать. Например, в задаче генерации текста после вновь сгенерированного слова пришлось бы перезапускать RNN-ячейку-справа-налево заново из-за необходимости учета нового слова. К тому же особого смысла эмбеддинги, получаемые этой ячейкой, не имели бы, постольку поскольку эмбеддинг $\\overrightarrow{y_T}$ будет иметь в себе информацию о всей последовательности.\n",
    "\n",
    "**Резюме главы**\n",
    "1. В RNN есть внутренние состояния, а сама сеть пропускает через себя входные эмбеддинги по одному.\n",
    "2. Выходом RNN является последовательность такой же длины, что и входная.\n",
    "3. RNN можно усложнить, сделав ее двунаправленной, но это не всегда возможно.\n",
    "4. RNN можно усложнить, составив в ней несколько слоев (на практике от 1 до 3 слоев).\n",
    "5. RNN слой состоит из RNN-ячеек.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внутреннее устройство RNN ячеек <a id='ssection_rnn_cells'></a>\n",
    "Как мы поняли, RNN слой состоит из нескольких (может быть и одной) RNN-ячеек. Теперь для полноты картины осталось понять, что происходит внутри ячейки RNN. Понятно, что внутри ячейки могут производится произвольные вычисления. На практике обычно используют 3 варианта реализации рекуррентной ячейки. Все эти реализации есть и в модуле `torch.nn`.\n",
    "\n",
    "Посмотрим, что происходит внутри ячейки с внутренним состоянием $h$ при обработке входного эмбеддинга $x$.\n",
    "\n",
    "На уже знакомом нам рисунке RNN-ячейка обозначена за $A$.\n",
    "![Unrolled RNN](images/rnn-unrolled.png \"Unrolled RNN\")\n",
    "\n",
    "### Ячейка \"RNN\" (`torch.nn.RNNCell`) <a id='sssection_rnn_cell'></a>\n",
    "Данная ячейка реализована в виде класса `torch.nn.RNNCell`.\n",
    "\n",
    "Имеет 4 обучаемых параметра: 2 матрицы $W_{ih} \\in \\mathbb{R}^{d_{hidden} \\times d_{in}}$, $W_{hh} \\in \\mathbb{R}^{d_{hidden} \\times d_{hidden}}$ и 2 вектора сдвигов (bias) $b_{ih} \\in \\mathbb{R}^{d_{hidden}}$, $b_{hh} \\in \\mathbb{R}^{d_{hidden}}$.  \n",
    "Матрица $W_{ih}$ и сдвиг $b_{ih}$ используются для трансформации входного вектора $x$, а матрица $W_{hh}$ и сдвиг $b_{hh}$ используются для трансформации внутреннего состояния ячейки $h$.  \n",
    "На выходе из ячейки имеем вектор $h^{\\prime}$ — новое внутренне состояние ячейки.\n",
    "\n",
    "Преобразование в ячейке описывается следующим выражением:\n",
    "\n",
    "$$\n",
    "h^{\\prime}=\\tanh \\left(W_{i h} x+b_{i h}+W_{h h} h+b_{h h}\\right).\n",
    "$$\n",
    "\n",
    "По сути к входному вектору и вектору внутреннего состояния ячейки применяются афинные преобразования, результаты преобразований складываются, после чего к итоговому вектору применяется нелинейность $\\tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$.\n",
    "\n",
    "### Ячейка \"Управляемый рекуррентный блок\" (GRU, `torch.nn.GRUCell`) <a id='sssection_gru_cell'></a>\n",
    "Данная ячейка реализована в виде класса `torch.nn.GRUCell`.\n",
    "\n",
    "Имеет 12 обучаемых параметров:\n",
    "- 3 матрицы для преобразования входного вектора $x$: $W_{ir}, W_{iz}, W_{in} \\in \\mathbb{R}^{d_{hidden} \\times d_{in}}$;\n",
    "- 3 матрицы для преобразования вектора внутреннего состояния ячейки $h$: $W_{hr}, W_{hz}, W_{hn} \\in \\mathbb{R}^{d_{hidden} \\times d_{hidden}}$;\n",
    "- 3 вектора сдвигов (bias) для преобразования входного вектора $x$: $b_{ir}, b_{iz}, b_{in} \\in \\mathbb{R}^{d_{in}}$;\n",
    "- 3 вектора сдвигов (bias) для преобразования вектора внутреннего состояния ячейки $h$: $b_{hr}, b_{hz}, b_{hn} \\in \\mathbb{R}^{d_{hidden}}$.\n",
    "\n",
    "Матрицы $W_{i ?}$ и сдвиги $b_{i ?}$ используются для трансформации входного вектора $x$, а матрицы $W_{h ?}$ и сдвиги $b_{h ?}$ используются для трансформации внутреннего состояния ячейки $h$. Здесь под знаком вопроса обозначена любая из букв $r, z$ и $n$.  \n",
    "На выходе из ячейки имеем вектор $h^{\\prime}$.\n",
    "\n",
    "Преобразования в ячейке описываются следующими выражениями:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&r=\\sigma\\left(W_{i r} x+b_{i r}+W_{h r} h+b_{h r}\\right) \\\\\n",
    "&z=\\sigma\\left(W_{i z} x+b_{i z}+W_{h z} h+b_{h z}\\right) \\\\\n",
    "&n=\\tanh \\left(W_{i n} x+b_{i n}+r *\\left(W_{h n} h+b_{h n}\\right)\\right) \\\\\n",
    "&h^{\\prime}=(1-z) * n+z * h\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![GRU cell](images/gru-cell.png \"GRU cell\")\n",
    "\n",
    "На схеме вектор $n$ обозначен за $\\tilde{h_t}$.\n",
    "\n",
    "Сначала в ячейке вычисляются два вектора $r$ и $z$. Они получаются аналогично тому, как в `RNNCell` вычисляется выход $h^{\\prime}$, только в конце применяется не $\\tanh$, а сигмоида $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$.\n",
    "\n",
    "Далее $z$ используется при расчете вектора $n$ — он вычисляется аналогично векторам $r$ и $z$ за исключением того, что преобразованное скрытое состояние домножается поточечно на вектор $r$ (а в конце применяется $\\tanh$, а не сигмоида). Таким образом, если все компоненты вектора $r$ будут близки к 0, то в векторе $n$ почти не будет никакой информации о прошлом. Если же компоненты вектора $r$ будут близки к 1, то в векторе $n$ информация о прошлом сохранится. По сути $z$ помогает модели определить, сколько уже выученной ячейкой информации нужно сохранить в векторе $n$.\n",
    "\n",
    "Далее выходом модели считается взвешенная сумма векторов $n$ и $h$ с коэффициентами $z$. Чем ближе компоненты вектора $z$ к 1, тем больше модель сохраняет свое старое состояние. Чем ближе компоненты вектора $z$ к 0, тем больше модель склонна к тотальному обновлению своего внутреннего состояния с $h$ на $n$.\n",
    "\n",
    "Во время работы ячейки 6 афинных преобразований (3 для $x$ и 3 для $h$) применяются к одним и тем же векторам. Для эффективности вычислений можно 3 афинных преобразования над вектором $x$ (аналогично и для вектора $h$) произвести одновременно с помощью одного матричного произведения вместо трех (подумайте, почему так будет быстрее!). Для этого:\n",
    "- Матрицы $W_{i?}$ объединеняют в одну $W_{ih} \\in \\mathbb{R}^{3d_{hidden} \\times d_{in}}$;\n",
    "- Матрицы $W_{h?}$ объединеняют в одну $W_{hh} \\in \\mathbb{R}^{3d_{hidden} \\times d_{hidden}}$;\n",
    "- Векторы $b_{i?}$ объединеняют в один $b_{ih} \\in \\mathbb{R}^{3d_{hidden}}$;\n",
    "- Векторы $b_{h?}$ объединеняют в один $b_{hh} \\in \\mathbb{R}^{3d_{hidden}}$;\n",
    "- Во время работы ячейки к входу $x$ и внутреннему состоянию $h$ применяются сразу все афинные преобразования с помощью матричного произведения $q$ = $W_{ih} x + b_{ih} + W_{hh} h + b_{hh}$.\n",
    "- После этого вектор $q \\in \\mathbb{R}^{3 d_{hidden}}$ разрезается на три части $r$, $z$, $n$. Все оставшиеся вычисления выполняются по формулам.\n",
    "\n",
    "В реализации `torch.nn.GRUCell` для эффективности вычислений применяется именно этот трюк.\n",
    "\n",
    "Подробнее про GRU сети можно [почитать тут](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be).\n",
    "\n",
    "### Ячейка \"Долгая краткосрочная память\" (LSTM, `torch.nn.LSTMCell`) <a id='sssection_lstm_cell'></a>\n",
    "Данная ячейка реализована в виде класса `torch.nn.LSTMCell`.\n",
    "\n",
    "Имеет 16 обучаемых параметров:\n",
    "- 4 матрицы для преобразования входного вектора $x$: $W_{ii}, W_{if}, W_{ig}, W_{io} \\in \\mathbb{R}^{d_{hidden} \\times d_{in}}$;\n",
    "- 4 матрицы для преобразования вектора внутреннего состояния ячейки $h$: $W_{hi}, W_{hf}, W_{hg}, W_{ho} \\in \\mathbb{R}^{d_{hidden} \\times d_{hidden}}$;\n",
    "- 4 вектора сдвигов (bias) для преобразования входного вектора $x$: $b_{ii}, b_{if}, b_{ig}, b_{io} \\in \\mathbb{R}^{d_{in}}$;\n",
    "- 4 вектора сдвигов (bias) для преобразования вектора внутреннего состояния ячейки $h$: $b_{hi}, b_{hf}, b_{hg}, b_{ho} \\in \\mathbb{R}^{d_{hidden}}$.\n",
    "\n",
    "Матрицы $W_{i ?}$ и сдвиги $b_{i ?}$ используются для трансформации входного вектора $x$, а матрицы $W_{h ?}$ и сдвиги $b_{h ?}$ используются для трансформации внутреннего состояния ячейки $h$. Здесь под знаком вопроса обозначена любая из букв $i, f, g$ и $o$.  \n",
    "На выходе из ячейки имеем вектор $h^{\\prime}$.\n",
    "\n",
    "Преобразования в ячейке описываются следующими выражениями:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&i=\\sigma\\left(W_{i i} x+b_{i i}+W_{h i} h+b_{h i}\\right) \\\\\n",
    "&f=\\sigma\\left(W_{i f} x+b_{i f}+W_{h f} h+b_{h f}\\right) \\\\\n",
    "&g=\\tanh \\left(W_{i g} x+b_{i g}+W_{h g} h+b_{h g}\\right) \\\\\n",
    "&o=\\sigma\\left(W_{i o} x+b_{i o}+W_{h o} h+b_{h o}\\right) \\\\\n",
    "&c^{\\prime}=f * c+i * g \\\\\n",
    "&h^{\\prime}=o * \\tanh \\left(c^{\\prime}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![LSTM cell](images/lstm-cell.png \"LSTM cell\")\n",
    "\n",
    "В `LSTMCell` в отличие от `RNNCell` и `GRUCell` используется дополнительный вектор внутреннего состояния $c$, отвечающий за долговременную память ячейки. Вектор $h$ называют краткосрочной памятью ячейки LSTM. По сути в ячейке LSTM за внутреннее состояние теперь отвечают два вектора — $h$ и $c$.\n",
    "\n",
    "Сначала в ячейке вычисляются четыре вектора $i, f, g$ и $o$. Они получаются аналогично тому, как в `RNNCell` вычисляется выход $h^{\\prime}$, только в конце применяется не $\\tanh$, а сигмоида для вычисления $i, f$ и $o$. Вектор $g$ считается кандидатом на новую долговременную память ячейки.\n",
    "\n",
    "Далее новая долговременная память $c^{\\prime}$ получается из прошлой долговременной памяти $c$ и кандидата $g$ с помощью взвешенной суммы с коэффициентами $f$ при $c$ и $i$ при $g$.  \n",
    "$f$ называют \"forget gate\" или вентилем забывания (на мой взгляд было бы логичнее называть его вентилем запоминания). Он отвечает за то, что перекочует из старой долгосрочной памяти в новую.  \n",
    "$i$ называют \"input gate\" или вентилем входа. Он отвечает за то, что добавится в новую долгосрочную память из кандидата $g$.\n",
    "\n",
    "Новое значение внутреннего состояния $h$ ячейки образуется поточечным домножением на $o$ вектора $h^{\\prime}$, к которому применили $\\tanh$.  \n",
    "$o$ называют \"output gate\" или вентилем выхода. Он отвечает за то, что добавится в новую кратковременную память из новой долгосрочной памяти.\n",
    "\n",
    "Во время работы ячейки 8 афинных преобразований (4 для $x$ и 4 для $h$) применяются к одним и тем же векторам. Для эффективности вычислений можно 4 афинных преобразования над вектором $x$ (аналогично и для вектора $h$) произвести одновременно с помощью одного матричного произведения вместо трех (подумайте, почему так будет быстрее!). Для этого:\n",
    "- Матрицы $W_{i?}$ объединеняют в одну $W_{ih} \\in \\mathbb{R}^{4d_{hidden} \\times d_{in}}$;\n",
    "- Матрицы $W_{h?}$ объединеняют в одну $W_{hh} \\in \\mathbb{R}^{4d_{hidden} \\times d_{hidden}}$;\n",
    "- Векторы $b_{i?}$ объединеняют в один $b_{ih} \\in \\mathbb{R}^{4d_{hidden}}$;\n",
    "- Векторы $b_{h?}$ объединеняют в один $b_{hh} \\in \\mathbb{R}^{4d_{hidden}}$;\n",
    "- Во время работы ячейки к входу $x$ и внутреннему состоянию $h$ применяются сразу все афинные преобразования с помощью матричного произведения $q$ = $W_{ih} x + b_{ih} + W_{hh} h + b_{hh}$.\n",
    "- После этого вектор $q \\in \\mathbb{R}^{4 d_{hidden}}$ разрезается на четыре части $i, f, g, o$. Все оставшиеся вычисления выполняются по формулам.\n",
    "\n",
    "В реализации `torch.nn.LSTMCell` для эффективности вычислений применяется именно этот трюк.\n",
    "\n",
    "Подробнее про LSTM сети можно [почитать тут](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "---\n",
    "\n",
    "На практике обычно не используют RNN-cell в силу ее простоты. GRU-cell и LSTM-cell обычно показывают схожее качество на реальных задачах.\n",
    "\n",
    "**Резюме главы**\n",
    "1. Есть несколько видов ячеек RNN: RNN-cell, GRU-cell, LSTM-cell.\n",
    "2. На практике используют GRU и LSTM сети.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-ячейки в `torch` <a id='ssection_torch_rnn_cells'></a>\n",
    "\n",
    "Давайте теперь подробнее посмотрим на работу с ячейками в коде. Будем использовать фреймворк для глубинного обучения `PyTorch`.\n",
    "\n",
    "Импортируем библиотеку `torch`. Функция `shape(tensor)` предназначена для красивого распечатывания размера тензора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch          # Импортируем библиотеку torch,\n",
    "from torch import nn  #  а также модуль nn для работы с нейронными сетями\n",
    "# Импорты библиотек были продублированы тут, чтобы можно было легко \n",
    "#  запускать куски этого ноутбука независимо друг от друга.\n",
    "\n",
    "\n",
    "def shape(tensor):\n",
    "    return ' x '.join(map(str, tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим $d_{in}$, $d_{hidden}$, размер батча для обработки (в коде будем использовать обозначение `batch_size`) и длину входной последовательности ($T$, в коде будем использовать обозначение `seq_len`).\n",
    "\n",
    "В нейронные сети данные обычно подаются батчами — небольшими кусками данных, в которых собрано сразу несколько последовательностей для обучения или инференса. Так делают для более высокой вычислительной эффективности нейронных сетей. У крупных моделей время обучения может составлять часы, дни, а иногда и недели, поэтому использование эффективных методов обучения моделей очень важно.\n",
    "\n",
    "Обычно последовательности в батче обрабатываются независимо друг от друга (исключение тут составляют слои батч-нормализации). Поэтому если говорить про вычисления в рекуррентных ячейках, то просто у каждого вектора данных ($x, h$ и $c$) появляется новая размерность, отвечающая за номер последовательности в батче.\n",
    "\n",
    "В RNN-ячейку мы передаем один очередной входной эмбеддинг для каждой из последовательностей в батче, а также соответствующие им внутренние состояния ($h$, $c$ в случае LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "d_in = 7\n",
    "d_hidden = 23\n",
    "seq_len = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем случайные значения для входных векторов $x$ и для начальных внутренних состояний $h$ и $c$ ($c$ будет нужен только для LSTM). Заметим, что у тензоров `h0` и `c0` нет размерности `seq_len`, потому что внутреннее состояние ячейки \"выделяется\" сразу на всю последовательность целиком, меняется по мере прохождения $x_1, \\dots, x_T$ через ячейку, и, следовательно, от длины не зависит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(seq_len, batch_size, d_in)\n",
    "h0 = torch.rand(batch_size, d_hidden)\n",
    "c0 = torch.rand(batch_size, d_hidden)\n",
    "\n",
    "run_args = (d_in, d_hidden, x, h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Небольшое пояснение.* Мы считаем, что в `x[i, j]` находится $i$-ый эмбеддинг токена $j$-ой последовательности, в `h0[j]` лежит начальное скрытое состояние ячейки, соответствующее последовательности `x[:, j]`, а в `c0[j]` лежит соответствующая последовательности `x[:, j]` начальная долгосрочная память ячейки (в случае LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы смотреть на работу различных RNN ячеек, напишем для этого удобную функцию `run_rnn_cell(...)`. Она распечатывает размеры тензоров входа, выхода, скрытых состояний и параметров ячейки.\n",
    "\n",
    "В циклах `for t, x_t in enumerate(x)` мы постепенно пропускаем вход $x$ через ячейку. Заметьте, что на вход ячейка принимает батч эмбеддингов для текущего слова из каждой последовательности, то есть в ячейку мы не передаем все последовательности сразу целиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn_cell(\n",
    "    rnn_cell_type: str, d_in: int, d_hidden: int,\n",
    "    x: torch.Tensor, h0: torch.Tensor, c0: torch.Tensor = None\n",
    "):\n",
    "    if hasattr(torch.nn, rnn_cell_type):\n",
    "        cell_class = getattr(torch.nn, rnn_cell_type)\n",
    "    else:\n",
    "        cell_class = globals()[rnn_cell_type]\n",
    "\n",
    "    cell = cell_class(d_in, d_hidden)\n",
    "\n",
    "    # Цикл по параметрам ячейки\n",
    "    print(f\"Размеры параметров ячейки {type(cell).__name__}\")\n",
    "    for name, p in cell.named_parameters():\n",
    "        print(f\"  * Параметр {name} имеет размер {shape(p)}\")\n",
    "\n",
    "    if isinstance(cell, (nn.RNNCell, nn.GRUCell)):\n",
    "        # ! RNNCell или GRUCell\n",
    "        # Печатаем размеры входа и выхода ячейки\n",
    "        print()\n",
    "        print(f\"Размер входа: {shape(x)}\")\n",
    "        print(f\"Размер изначального скрытого состояния: {shape(h0)}\")\n",
    "        print()\n",
    "\n",
    "        # Пропускаем x и h0 через ячейку\n",
    "        h = h0\n",
    "        for t, x_t in enumerate(x):\n",
    "            h = cell(x_t, h)\n",
    "            print(f\"Размер скрытого состояния после шага {t + 1}: {shape(h)}\")\n",
    "    else:\n",
    "        # ! LSTMCell или MyLSTMCell\n",
    "        # Печатаем размеры входа и выхода ячейки\n",
    "        print()\n",
    "        print(f\"Размер входа: {shape(x)}\")\n",
    "        print(f\"Размер изначальной краткосрочной памяти: {shape(h0)}\")\n",
    "        print(f\"Размер изначальной  долгосрочной памяти: {shape(c0)}\")\n",
    "        print()\n",
    "\n",
    "        # Пропускаем x, h0 и c0 через ячейку\n",
    "        h, c = h0, c0\n",
    "        for t, x_t in enumerate(x):\n",
    "            h, c = cell(x_t, (h, c))\n",
    "\n",
    "            print(f\"Размер краткосрочной памяти после шага {t + 1}: {shape(h)}\")\n",
    "            print(f\"Размер  долгосрочной памяти после шага {t + 1}: {shape(c)}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `RNNCell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров ячейки RNNCell\n",
      "  * Параметр weight_ih имеет размер 23 x 7\n",
      "  * Параметр weight_hh имеет размер 23 x 23\n",
      "  * Параметр bias_ih имеет размер 23\n",
      "  * Параметр bias_hh имеет размер 23\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначального скрытого состояния: 3 x 23\n",
      "\n",
      "Размер скрытого состояния после шага 1: 3 x 23\n",
      "Размер скрытого состояния после шага 2: 3 x 23\n",
      "Размер скрытого состояния после шага 3: 3 x 23\n",
      "Размер скрытого состояния после шага 4: 3 x 23\n",
      "Размер скрытого состояния после шага 5: 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn_cell(\"RNNCell\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `GRUCell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров ячейки GRUCell\n",
      "  * Параметр weight_ih имеет размер 69 x 7\n",
      "  * Параметр weight_hh имеет размер 69 x 23\n",
      "  * Параметр bias_ih имеет размер 69\n",
      "  * Параметр bias_hh имеет размер 69\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначального скрытого состояния: 3 x 23\n",
      "\n",
      "Размер скрытого состояния после шага 1: 3 x 23\n",
      "Размер скрытого состояния после шага 2: 3 x 23\n",
      "Размер скрытого состояния после шага 3: 3 x 23\n",
      "Размер скрытого состояния после шага 4: 3 x 23\n",
      "Размер скрытого состояния после шага 5: 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn_cell(\"GRUCell\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `LSTMCell`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров ячейки LSTMCell\n",
      "  * Параметр weight_ih имеет размер 92 x 7\n",
      "  * Параметр weight_hh имеет размер 92 x 23\n",
      "  * Параметр bias_ih имеет размер 92\n",
      "  * Параметр bias_hh имеет размер 92\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначальной краткосрочной памяти: 3 x 23\n",
      "Размер изначальной  долгосрочной памяти: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 1: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 1: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 2: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 2: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 3: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 3: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 4: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 4: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 5: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 5: 3 x 23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_rnn_cell(\"LSTMCell\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, в ячейках `GRUCell` и `LSTMCell` действительно матрицы афинных преобразований склеены, а также видим, что в `GRUCell` склеено 3 матрицы, а в `LSTMCell` — 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа ячеек `RNNCell`, `GRUCell` и `LSTMCell` реализована на C/C++ и Cuda для обеспечения высокой эффективности их работы. Ниже представлена возможная реализация `LSTMCell` на `Python` для более глубокого ознакомления читателя с возможностями фреймворка `torch` и работой `LSTMCell`.\n",
    "\n",
    "Замечу, что в реализации в библиотеке `torch` используются дублирующие векторы сдвигов (bias) для совместимости с библиотекой cuDNN — CUDA Deep Neural Network (а точнее, для прямого вызова функций из cuDNN). Например, в $f = \\sigma\\left(W_{i f} x+b_{i f}+W_{h f} h+b_{h f}\\right)$ понятно, что оба вектора $b_{if}$ и $b_{hf}$ не нужны и их можно заменить на один вектор $b_{f} \\in \\mathbb{R}^{d_{hidden}}$. Поэтому в предлагаемой реализации создается всего один вектор для сдвига вместо двух для большей эффективности вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class MyLSTMCell(nn.Module):\n",
    "    def __init__(self, d_in: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_in = d_in\n",
    "        self.d_hidden = d_hidden\n",
    "\n",
    "        # Регистрируем self.weight_ih, self.weight_hh и self.bias_hh\n",
    "        #  в качестве параметров нашего MyLSTMCell модуля. Это нужно для\n",
    "        #  того, чтобы эти параметры обучались во время обучения.\n",
    "        self.weight_ih = torch.nn.Parameter(\n",
    "            data=torch.zeros(d_in, 4 * d_hidden)\n",
    "        )\n",
    "\n",
    "        self.weight_hh = torch.nn.Parameter(\n",
    "            data=torch.zeros(d_hidden, 4 * d_hidden)\n",
    "        )\n",
    "\n",
    "        self.bias_h = torch.nn.Parameter(\n",
    "            data=torch.zeros(4 * d_hidden)\n",
    "        )\n",
    "\n",
    "        # Можно создать и два вектора для сдвига, если очень хочется:\n",
    "        # self.bias_ih = torch.nn.Parameter(\n",
    "        #     data=torch.zeros(4 * d_hidden)\n",
    "        # )\n",
    "        # self.bias_hh = torch.nn.Parameter(\n",
    "        #     data=torch.zeros(4 * d_hidden)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h0_c0: Tuple[torch.Tensor, torch.Tensor]):\n",
    "        h0, c0 = h0_c0\n",
    "\n",
    "        Wx = torch.matmul(x, self.weight_ih)\n",
    "        Uh = torch.matmul(h0, self.weight_hh)\n",
    "\n",
    "        fioc = Wx + Uh + self.bias_h  # batch_size x 4*d_hidden\n",
    "        f, i, o, c = torch.chunk(fioc, 4, dim=1)\n",
    "        f, i, o = map(torch.sigmoid, [f, i, o])\n",
    "        c = torch.tanh(c)\n",
    "\n",
    "        c1 = f * c0 + i * c\n",
    "        h1 = o * torch.tanh(c1)\n",
    "\n",
    "        return h1, c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров ячейки MyLSTMCell\n",
      "  * Параметр weight_ih имеет размер 7 x 92\n",
      "  * Параметр weight_hh имеет размер 23 x 92\n",
      "  * Параметр bias_h имеет размер 92\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначальной краткосрочной памяти: 3 x 23\n",
      "Размер изначальной  долгосрочной памяти: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 1: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 1: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 2: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 2: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 3: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 3: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 4: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 4: 3 x 23\n",
      "\n",
      "Размер краткосрочной памяти после шага 5: 3 x 23\n",
      "Размер  долгосрочной памяти после шага 5: 3 x 23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_rnn_cell(\"MyLSTMCell\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюме главы**\n",
    "1. Изучили работу RNN-ячеек в `torch`;\n",
    "2. Рассмотрели реализацию LSTM-ячейки на `torch`.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN слои в `torch` <a id='ssection_torch_rnn'></a>\n",
    "\n",
    "В библиотеке `torch` помимио RNN-ячеек реализованы также и полноценные RNN-слои:\n",
    "- `torch.nn.RNN`\n",
    "- `torch.nn.GRU`\n",
    "- `torch.nn.LSTM`\n",
    "\n",
    "Отличаются они между собой только тем, какие внутри них используются RNN-ячейки.\n",
    "\n",
    "При создании каждого из этих слоев можно задать следующие параметры:\n",
    "1. `input_size` — размерность эмбеддингов входа ($d_{in}$);\n",
    "2. `hidden_size` — размерность внутреннего состояния ($d_{hidden}$);\n",
    "3. `num_layers` — количество слоев в сети;\n",
    "4. `bidirectional` — является ли сеть двунаправленной или нет;\n",
    "5. `bias` — использовать ли сдвиг при афинных преобразованиях;\n",
    "6. `batch_first` — можно задавать, в какой последовательности будут идти размерности входного и выходного тензоров. В случае `batch_first = False` предполагается, что первая размерность $x$ отвечает за длину последовательности, вторая размерность — за отдельные последовательности в батче, третья — за размерность эмбеддингов. В случае `batch_first = True` предполагается, что первая и вторая размерности $x$ поменяны местами, то есть, что первая размерность $x$ отвечает за отдельные последовательности в батче, вторая размерность — за длину последовательности, третья — за размерность эмбеддингов. Обычно используют вариант с `batch_first = False`.\n",
    "\n",
    "Также у RNN слоев есть еще и другие параметры. Тут перечислены пока что самые важные параметры для вашего понимания.\n",
    "\n",
    "Во время работы RNN слой принимает на вход сразу все `batch_size` последовательностей целиком, прокручивает их через ячейки и возвращает следующее:\n",
    "1. Результат `output` работы слоя в виде тензора размера $T \\times \\text{batch_size} \\times D \\cdot d_{hidden}$, где $D = 1$, если сеть однонаправленная, и $D = 2$, если сеть двунаправленная.\n",
    "2. Внутренние состояния `h_n` для каждой последовательности в батче и каждого отдельного слоя. `h_n` имеет размеры $D \\cdot \\text{num_layers} \\times \\text{batch_size} \\times H_{hidden}$.\n",
    "\n",
    "Таким образом на выходе из сети имеем тензор `output`, совпадающий по описанию с последовательностью $y_1, \\dots, y_T$ из [главы про функционирование RNN](#ssection_rnn_functioning).\n",
    "\n",
    "Работа слоев `RNN`, `GRU` и `LSTM` реализована на C/C++ и Cuda/cuDNN для обеспечения высокой эффективности их работы. Из-за этого реализация RNN слоев на Python с использованием RNN-ячеек будет работать медленнее, поэтому RNN-ячейки следует использовать только в случае надобности и когда вы понимаете, что делаете.\n",
    "\n",
    "**Резюме главы**\n",
    "1. В `torch` есть реализованные RNN слои, полностью реализующие рассмотренный функционал рекуррентных нейронных сетей.\n",
    "2. Не стоит переизобретать RNN слои без надобности.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример использования RNN слоев в `torch` <a id='ssection_torch_rnn_example'></a>\n",
    "\n",
    "Давайте теперь подробнее посмотрим на работу с RNN в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch          # Импортируем библиотеку torch,\n",
    "from torch import nn  #  а также модуль nn для работы с нейронными сетями\n",
    "# Импорты библиотек были продублированы тут, чтобы можно было легко \n",
    "#  запускать куски этого ноутбука независимо друг от друга.\n",
    "\n",
    "\n",
    "def shape(tensor):\n",
    "    return ' x '.join(map(str, tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим $d_{in}$, $d_{hidden}$, размер батча для обработки (в коде будем использовать обозначение `batch_size`), длину входной последовательности ($T$, в коде будем использовать обозначение `seq_len`), а также параметры RNN: количество слоев `num_layers` и индикатор двунаправленности сети `bidirectional`.\n",
    "\n",
    "В RNN слой мы передаем сразу все входные эмбеддинги для каждой из последовательностей в батче, а также соответствующие им внутренние состояния ($h$, $c$ в случае LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "d_in = 7\n",
    "d_hidden = 23\n",
    "seq_len = 5\n",
    "\n",
    "num_layers = 2\n",
    "bidirectional = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем случайные значения для входных векторов $x$ и для начальных внутренних состояний $h$ и $c$ ($c$ будет нужен только для LSTM). Заметим, что у тензоров `h0` и `c0` нет размерности `seq_len`, потому что внутреннее состояние ячейки \"выделяется\" сразу на всю последовательность целиком, меняется по мере прохождения $x_1, \\dots, x_T$ через ячейку, и, следовательно, от длины не зависит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2 if bidirectional else 1\n",
    "\n",
    "x = torch.rand(seq_len, batch_size, d_in)\n",
    "h0 = torch.rand(D * num_layers, batch_size, d_hidden)\n",
    "c0 = torch.rand(D * num_layers, batch_size, d_hidden)\n",
    "\n",
    "run_args = (d_in, d_hidden, num_layers, bidirectional, x, h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Небольшое пояснение.* Мы считаем, что в `x[i, j]` находится $i$-ый эмбеддинг токена $j$-ой последовательности, в `h0[l, j]` лежит начальное скрытое состояние слоя $l$, соответствующее последовательности `x[:, j]`, а в `c0[l, j]` лежит соответствующая последовательности `x[:, j]` начальная долгосрочная память слоя $l$ (в случае LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, что бы смотреть на работу различных RNN слоев, опишем для этого удобную функцию `run_rnn(...)`. Она распечатывает размеры тензоров входа, выхода, скрытых состояний и параметров слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rnn(\n",
    "    rnn_type: str, d_in: int, d_hidden: int,\n",
    "    num_layers: int, bidirectional: bool,\n",
    "    x: torch.Tensor, h0: torch.Tensor, c0: torch.Tensor = None\n",
    "):\n",
    "    rnn_class = getattr(torch.nn, rnn_type)\n",
    "    rnn = rnn_class(d_in, d_hidden, num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    # Цикл по параметрам RNN\n",
    "    print(f\"Размеры параметров слоя {type(rnn).__name__}\"\n",
    "          f\" ({num_layers} слоя, двунаправленная: {bidirectional})\")\n",
    "    for name, p in rnn.named_parameters():\n",
    "        print(f\"  * Параметр {name} имеет размер {shape(p)}\")\n",
    "\n",
    "    if isinstance(rnn, torch.nn.LSTM):\n",
    "        # ! LSTM\n",
    "        # Печатаем размеры входа и выхода ячейки\n",
    "        print()\n",
    "        print(f\"Размер входа: {shape(x)}\")\n",
    "        print(f\"Размер изначальной краткосрочной памяти: {shape(h0)}\")\n",
    "        print(f\"Размер изначальной  долгосрочной памяти: {shape(c0)}\")\n",
    "        print()\n",
    "\n",
    "        # Пропускаем x, h0 и c0 через ячейку\n",
    "        output, (hn, cn) = rnn(x, (h0, c0))\n",
    "        print(f\"Размер выхода: {shape(output)}\")\n",
    "        print(f\"Размер конечной краткосрочной памяти: {shape(hn)}\")\n",
    "        print(f\"Размер конечной  долгосрочной памяти: {shape(cn)}\")\n",
    "    else:\n",
    "        # ! RNN или GRU\n",
    "        # Печатаем размеры входа и выхода ячейки\n",
    "        print()\n",
    "        print(f\"Размер входа: {shape(x)}\")\n",
    "        print(f\"Размер изначального скрытого состояния: {shape(h0)}\")\n",
    "        print()\n",
    "\n",
    "        # Пропускаем x и h0 через ячейку\n",
    "        output, hn = rnn(x, h0)\n",
    "        print(f\"Размер выхода: {shape(output)}\")\n",
    "        print(f\"Размер конечного скрытого состояния: {shape(hn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `RNN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров слоя RNN (2 слоя, двунаправленная: False)\n",
      "  * Параметр weight_ih_l0 имеет размер 23 x 7\n",
      "  * Параметр weight_hh_l0 имеет размер 23 x 23\n",
      "  * Параметр bias_ih_l0 имеет размер 23\n",
      "  * Параметр bias_hh_l0 имеет размер 23\n",
      "  * Параметр weight_ih_l1 имеет размер 23 x 23\n",
      "  * Параметр weight_hh_l1 имеет размер 23 x 23\n",
      "  * Параметр bias_ih_l1 имеет размер 23\n",
      "  * Параметр bias_hh_l1 имеет размер 23\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначального скрытого состояния: 2 x 3 x 23\n",
      "\n",
      "Размер выхода: 5 x 3 x 23\n",
      "Размер конечного скрытого состояния: 2 x 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn(\"RNN\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `GRU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров слоя GRU (2 слоя, двунаправленная: False)\n",
      "  * Параметр weight_ih_l0 имеет размер 69 x 7\n",
      "  * Параметр weight_hh_l0 имеет размер 69 x 23\n",
      "  * Параметр bias_ih_l0 имеет размер 69\n",
      "  * Параметр bias_hh_l0 имеет размер 69\n",
      "  * Параметр weight_ih_l1 имеет размер 69 x 23\n",
      "  * Параметр weight_hh_l1 имеет размер 69 x 23\n",
      "  * Параметр bias_ih_l1 имеет размер 69\n",
      "  * Параметр bias_hh_l1 имеет размер 69\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначального скрытого состояния: 2 x 3 x 23\n",
      "\n",
      "Размер выхода: 5 x 3 x 23\n",
      "Размер конечного скрытого состояния: 2 x 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn(\"GRU\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `LSTM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров слоя LSTM (2 слоя, двунаправленная: False)\n",
      "  * Параметр weight_ih_l0 имеет размер 92 x 7\n",
      "  * Параметр weight_hh_l0 имеет размер 92 x 23\n",
      "  * Параметр bias_ih_l0 имеет размер 92\n",
      "  * Параметр bias_hh_l0 имеет размер 92\n",
      "  * Параметр weight_ih_l1 имеет размер 92 x 23\n",
      "  * Параметр weight_hh_l1 имеет размер 92 x 23\n",
      "  * Параметр bias_ih_l1 имеет размер 92\n",
      "  * Параметр bias_hh_l1 имеет размер 92\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначальной краткосрочной памяти: 2 x 3 x 23\n",
      "Размер изначальной  долгосрочной памяти: 2 x 3 x 23\n",
      "\n",
      "Размер выхода: 5 x 3 x 23\n",
      "Размер конечной краткосрочной памяти: 2 x 3 x 23\n",
      "Размер конечной  долгосрочной памяти: 2 x 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn(\"LSTM\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, в двух-слойных RNN действительно содержится по 2 RNN-ячейки.\n",
    "\n",
    "Попробуем теперь добавить еще двунаправленность для RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2 if bidirectional else 1\n",
    "\n",
    "x = torch.rand(seq_len, batch_size, d_in)\n",
    "h0 = torch.rand(D * num_layers, batch_size, d_hidden)\n",
    "c0 = torch.rand(D * num_layers, batch_size, d_hidden)\n",
    "\n",
    "run_args = (d_in, d_hidden, num_layers, bidirectional, x, h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `RNN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров слоя RNN (2 слоя, двунаправленная: True)\n",
      "  * Параметр weight_ih_l0 имеет размер 23 x 7\n",
      "  * Параметр weight_hh_l0 имеет размер 23 x 23\n",
      "  * Параметр bias_ih_l0 имеет размер 23\n",
      "  * Параметр bias_hh_l0 имеет размер 23\n",
      "  * Параметр weight_ih_l0_reverse имеет размер 23 x 7\n",
      "  * Параметр weight_hh_l0_reverse имеет размер 23 x 23\n",
      "  * Параметр bias_ih_l0_reverse имеет размер 23\n",
      "  * Параметр bias_hh_l0_reverse имеет размер 23\n",
      "  * Параметр weight_ih_l1 имеет размер 23 x 46\n",
      "  * Параметр weight_hh_l1 имеет размер 23 x 23\n",
      "  * Параметр bias_ih_l1 имеет размер 23\n",
      "  * Параметр bias_hh_l1 имеет размер 23\n",
      "  * Параметр weight_ih_l1_reverse имеет размер 23 x 46\n",
      "  * Параметр weight_hh_l1_reverse имеет размер 23 x 23\n",
      "  * Параметр bias_ih_l1_reverse имеет размер 23\n",
      "  * Параметр bias_hh_l1_reverse имеет размер 23\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначального скрытого состояния: 4 x 3 x 23\n",
      "\n",
      "Размер выхода: 5 x 3 x 46\n",
      "Размер конечного скрытого состояния: 4 x 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn(\"RNN\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `GRU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров слоя GRU (2 слоя, двунаправленная: True)\n",
      "  * Параметр weight_ih_l0 имеет размер 69 x 7\n",
      "  * Параметр weight_hh_l0 имеет размер 69 x 23\n",
      "  * Параметр bias_ih_l0 имеет размер 69\n",
      "  * Параметр bias_hh_l0 имеет размер 69\n",
      "  * Параметр weight_ih_l0_reverse имеет размер 69 x 7\n",
      "  * Параметр weight_hh_l0_reverse имеет размер 69 x 23\n",
      "  * Параметр bias_ih_l0_reverse имеет размер 69\n",
      "  * Параметр bias_hh_l0_reverse имеет размер 69\n",
      "  * Параметр weight_ih_l1 имеет размер 69 x 46\n",
      "  * Параметр weight_hh_l1 имеет размер 69 x 23\n",
      "  * Параметр bias_ih_l1 имеет размер 69\n",
      "  * Параметр bias_hh_l1 имеет размер 69\n",
      "  * Параметр weight_ih_l1_reverse имеет размер 69 x 46\n",
      "  * Параметр weight_hh_l1_reverse имеет размер 69 x 23\n",
      "  * Параметр bias_ih_l1_reverse имеет размер 69\n",
      "  * Параметр bias_hh_l1_reverse имеет размер 69\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначального скрытого состояния: 4 x 3 x 23\n",
      "\n",
      "Размер выхода: 5 x 3 x 46\n",
      "Размер конечного скрытого состояния: 4 x 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn(\"GRU\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу `LSTM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры параметров слоя LSTM (2 слоя, двунаправленная: True)\n",
      "  * Параметр weight_ih_l0 имеет размер 92 x 7\n",
      "  * Параметр weight_hh_l0 имеет размер 92 x 23\n",
      "  * Параметр bias_ih_l0 имеет размер 92\n",
      "  * Параметр bias_hh_l0 имеет размер 92\n",
      "  * Параметр weight_ih_l0_reverse имеет размер 92 x 7\n",
      "  * Параметр weight_hh_l0_reverse имеет размер 92 x 23\n",
      "  * Параметр bias_ih_l0_reverse имеет размер 92\n",
      "  * Параметр bias_hh_l0_reverse имеет размер 92\n",
      "  * Параметр weight_ih_l1 имеет размер 92 x 46\n",
      "  * Параметр weight_hh_l1 имеет размер 92 x 23\n",
      "  * Параметр bias_ih_l1 имеет размер 92\n",
      "  * Параметр bias_hh_l1 имеет размер 92\n",
      "  * Параметр weight_ih_l1_reverse имеет размер 92 x 46\n",
      "  * Параметр weight_hh_l1_reverse имеет размер 92 x 23\n",
      "  * Параметр bias_ih_l1_reverse имеет размер 92\n",
      "  * Параметр bias_hh_l1_reverse имеет размер 92\n",
      "\n",
      "Размер входа: 5 x 3 x 7\n",
      "Размер изначальной краткосрочной памяти: 4 x 3 x 23\n",
      "Размер изначальной  долгосрочной памяти: 4 x 3 x 23\n",
      "\n",
      "Размер выхода: 5 x 3 x 46\n",
      "Размер конечной краткосрочной памяти: 4 x 3 x 23\n",
      "Размер конечной  долгосрочной памяти: 4 x 3 x 23\n"
     ]
    }
   ],
   "source": [
    "run_rnn(\"LSTM\", *run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что в сетях появились параметры с суффиксом `_reverse` — это как раз ячейки, которые отвечают за чтение входа справа-налево. Также интересно тут поразглядывать размерности параметров промежуточных слоев. Например, в LSTM параметр `weight_ih_l1` имеет размерности $4 \\cdot d_{hidden} \\times 2 \\cdot d_{hidden}$, потому что на вход он получает с предыдущего слоя (с индексом 0) комбинированный вектор $y_t^0$, полученный после конкатенации векторов $\\overrightarrow{y_t}^{0}$ и $\\overleftarrow{y_t}^{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**По итогу хотелось бы еще раз повторить:**\n",
    "1. На практике стоит использовать `LSTM` или `GRU` сети с 1-3 слоями.\n",
    "2. Двунаправленность обычно позволяет улучшить качество решения задачи, но из-за нее увеличивается количество обучаемых параметров более, чем вдвое. К тому же иногда нет смысла делать RNN двунаправленной.\n",
    "3. RNN-<u>ячейки</u> не стоит использовать без необходимости.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Упаковка последовательностей разной длины <a id='ssection_sequences_packing'></a>\n",
    "Возможно, вы, пока читали этот сопроводительный ноутбук, задались вопросом, как мы можем запихнуть в один батч несколько последовательностей разной длины?\n",
    "Напомню, что необходимость класть несколько последовательностей в один батч у нас появилась во время экспериментов над RNN слоями.\n",
    "\n",
    "### Упаковка последовательности <a id='sssection_sequences_packing_filtration'></a>\n",
    "В батче для *RNN-ячеек* мы хранили лишь по одному эмбеддингу из каждой последовательности, поэтому при работе с ними у нас была только необходимость положить в батч эмбеддинги для текущего слова из каждой последовательности, а с этим проблем не возникает. Единственное тонкое место тут, а что делать, если в каких-то последовательностях закончились слова, а в каких-то — нет? Тогда мы можем сформировать батч только из эмбеддингов тех последовательностей, в которых слова еще не кончились, и подать их на вход сети. Тут надо аккуратно отфильтровать еще и внутренние состояния сети для подачи их в ячейку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся к примеру, чтобы стало понятнее о чем идет речь. Для начала импортируем `torch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch          # Импортируем библиотеку torch,\n",
    "from torch import nn  #  а также модуль nn для работы с нейронными сетями\n",
    "# Импорты библиотек были продублированы тут, чтобы можно было легко \n",
    "#  запускать куски этого ноутбука независимо друг от друга.\n",
    "\n",
    "def shape(tensor):\n",
    "    return ' x '.join(map(str, tensor.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список предложений, которые будем сейчас пропускать через ячейку RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    [\"hey\"],\n",
    "    [\"deadline\", \"is\", \"tomorrow\"],\n",
    "    [\"so\", \"have\", \"you\", \"started\", \"doing\", \"anything\"],\n",
    "    [\"what\"],\n",
    "    [\"oh\", \"sure\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим по тексту словарь и отображение из слов в индексы (код их создания не приводится для того, чтобы вы написали его самостоятельно). В целом, словарь нам не очень нужен, но с помощью него проще создавать отображение слов в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {\n",
    "    \"anything\",\n",
    "    \"deadline\",\n",
    "    \"doing\",\n",
    "    \"have\",\n",
    "    \"hey\",\n",
    "    \"is\",\n",
    "    \"oh\",\n",
    "    \"so\",\n",
    "    \"started\",\n",
    "    \"sure\",\n",
    "    \"tomorrow\",\n",
    "    \"what\",\n",
    "    \"you\"\n",
    "}\n",
    "\n",
    "word2ind = {\n",
    "    \"anything\": 0,\n",
    "    \"deadline\": 1,\n",
    "    \"doing\": 2,\n",
    "    \"have\": 3,\n",
    "    \"hey\": 4,\n",
    "    \"is\": 5,\n",
    "    \"oh\": 6,\n",
    "    \"so\": 7,\n",
    "    \"started\": 8,\n",
    "    \"sure\": 9,\n",
    "    \"tomorrow\": 10,\n",
    "    \"what\": 11,\n",
    "    \"you\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим предложения в индексы (код не приводится для того, чтобы вы написали его самостоятельно):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_idxs = [\n",
    "    [4], \n",
    "    [1, 5, 10], \n",
    "    [7, 3, 12, 8, 2, 0], \n",
    "    [11], \n",
    "    [6, 9]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим слой уже знакомых вам эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(13, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_in = 2      # Размерность эмбеддингов\n",
    "d_hidden = 10 # Размерность скрытого состояния ячеек\n",
    "\n",
    "embeddings = nn.Embedding(num_embeddings=len(vocabulary), embedding_dim=d_in)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем цикл обработки предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущий батч индексов слов: tensor([ 4,  1,  7, 11,  6])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [True, True, True, True, True]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([5, 3, 9])\n",
      "Батч эмбеддингов для текущих слов имеет размер 3 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  3 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, True, True, False, True]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, True, True, False, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([10, 12])\n",
      "Батч эмбеддингов для текущих слов имеет размер 2 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  2 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, True, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, True, True, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([8])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, False, True, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([2])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, False, True, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([0])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, False, True, False, False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Будем сюда сохранять итоговые состояния ячеек. Далее будет еще\n",
    "#  несколько примеров реализации того, что написано в этой клетке.\n",
    "#  В конце мы проверим, что все внутренние состояния в различных\n",
    "#  реализациях совпадают.\n",
    "hs = []\n",
    "\n",
    "# Зададим зерно для генератора псевдослучайных чисел (ГПСЧ)\n",
    "#  для воспроизводимости результатов, а именно чтобы всегда\n",
    "#  RNN-ячейка и внутреннее состояние генерировались одинаковыми\n",
    "#  от запуска к запуску.\n",
    "torch.manual_seed(179)\n",
    "\n",
    "# Создаем ячейки и инициализируем случайно внутренние состояния ячейки.\n",
    "cell = nn.RNNCell(d_in, d_hidden)\n",
    "h = torch.rand(len(texts_idxs), d_hidden)\n",
    "\n",
    "length_longest_seq = max(map(len, texts_idxs))\n",
    "\n",
    "# Цикл по длине последовательностей\n",
    "for step in range(length_longest_seq):\n",
    "    # Определим последовательности, в которых слова еще не кончились.\n",
    "    mask_leave = [len(text_idxs) > step for text_idxs in texts_idxs]\n",
    "\n",
    "    # Создадим батч из текущих слов\n",
    "    batch = [\n",
    "        text_idxs[step]\n",
    "        for leave, text_idxs\n",
    "        in zip(mask_leave, texts_idxs)\n",
    "        if leave\n",
    "    ]\n",
    "\n",
    "    # Переводим батч в тензор\n",
    "    batch = torch.LongTensor(batch)\n",
    "    print(f\"Текущий батч индексов слов: {batch}\")\n",
    "\n",
    "    # Получаем для него эмбеддинги\n",
    "    embeds = embeddings(batch)\n",
    "    print(f\"Батч эмбеддингов для текущих слов имеет размер {shape(embeds)}\")\n",
    "\n",
    "    h_prev = h.clone()\n",
    "\n",
    "    # Подаем RNN ячейке на вход эмбеддинги и\n",
    "    #  соответствующие им внутренние состояния ячейки\n",
    "    h[mask_leave] = cell(embeds, h[mask_leave])\n",
    "    print(f\"Внутренние состояния для последовательностей,\\n\"\n",
    "          f\"  которые пропускались через RNN, имеют размер  {shape(h[mask_leave])}\")\n",
    "    print(f\"Маска последовательностей, которые пока что еще пропускаются\\n\"\n",
    "          f\"  через RNN-ячейку:     {mask_leave}\")\n",
    "    print(f\"Проверка того, для каких последовательностей изменились\\n\"\n",
    "          f\"  внутренние состояния: {(h != h_prev).any(dim=1).tolist()}\")\n",
    "    print()\n",
    "\n",
    "hs.append(h.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выводе ячейки выше можно пронаблюдать, как постепенно предложения пропускаются через `RNNCell`. Чтобы упростить понимание того, что происходит, можно обратиться к иллюстрации упаковки последовательностей:\n",
    "\n",
    "![Packed or padded sequences](images/packed-padded-sequences.jpeg \"Packed or padded sequences\")\n",
    "На рисунке последовательности обозначены черным. Последовательности начинаются слева и заканчиваются справа, а сверху вниз приведены несколько последовательностей. Цветами выделены батчи. Справа на рисунке изображены размеры батчей на каждом шаге."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Несколько трюков <a id='ssssection_sequences_packing_filtration_tricks'></a>\n",
    "При создании батча по маске можно было бы использовать функцию `compress` из стандартной библиотеки `itertools`. Данная функция осуществляет маскирование для списков.\n",
    "```python\n",
    "batch = [\n",
    "    text_idxs[step]\n",
    "    for leave, text_idxs\n",
    "    in zip(mask_leave, texts_idxs)\n",
    "    if leave\n",
    "]\n",
    "```\n",
    "\n",
    "$\\rightarrow$\n",
    "\n",
    "```python\n",
    "batch = [\n",
    "    text_idxs[step] for text_idxs\n",
    "    in compress(texts_idxs, mask_leave)\n",
    "]\n",
    "```\n",
    "\n",
    "Пример работы этой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import compress\n",
    "\n",
    "\n",
    "list(compress([1, 2, 3, 4], [True, False, False, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также сам способ образования батчей можно было бы возложить на стандартную функцию `zip`. С помощью этой функции можно транспонировать вложенные объекты в питоне. Чтобы стало понятно, о чем идет речь, приведем следующий пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(2, 5)\n",
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "a = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "\n",
    "a_transposed = list(zip(*a))\n",
    "\n",
    "for row in a_transposed:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, почему `zip(*a)` работает так, как работает, немного подумайте об этом сами ;)\n",
    "\n",
    "<details>\n",
    "<summary><font color='blue'>Нажмите сюда для объяснения</font></summary>\n",
    "\n",
    "Мы по сути передаем в функцию `zip` все строки нашей \"матрицы\". Функция `zip` выдает нам склеенные первые элементы из каждой строки, затем выдает склеенные вторые элементы из каждой строки и так далее до конца каждой из строк. Поэтому на выходе мы имеем сначала первый столбец матрицы, затем второй и т.д. до последнего столбца. А математически мы тут получили как раз транспонирование матрицы.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А мы как раз и транспонируем список `texts_idxs`. Проблема с ним в том, что в нем строки разной длины (мы как раз сейчас здесь из-за этого и страдаем). Функция `zip` транспонирует только самые длинные \"столбцы\" в нашем списке, потому что `zip` выдает столбцы до тех пор, пока не кончатся элементы хотя бы в одной последовательности. Для того, чтобы побороться с этим, будем использовать функцию `zip_longest` из стандартной библиотеки `itertools`. Эта функция будет выдавать строки, пока хотя бы в одной строке еще есть элементы. Строки, в которых элементы уже кончились, дополняются `None` до длины самой длинной строки.\n",
    "\n",
    "Посмотрим на работу этой функции на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'a', 9)\n",
      "(2, 'b', 8)\n",
      "(3, None, 7)\n",
      "(None, None, 6)\n",
      "(None, None, 5)\n"
     ]
    }
   ],
   "source": [
    "from itertools import zip_longest\n",
    "\n",
    "\n",
    "for col in zip_longest(\n",
    "    [1, 2, 3],\n",
    "    \"ab\",\n",
    "    [9, 8, 7, 6, 5]\n",
    "):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С учетом замечаний выше перепишем цикл обработки предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Неочищенный батч: (4, 1, 7, 11, 6)\n",
      "Текущий батч индексов слов: tensor([ 4,  1,  7, 11,  6])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [True, True, True, True, True]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Неочищенный батч: (None, 5, 3, None, 9)\n",
      "Текущий батч индексов слов: tensor([5, 3, 9])\n",
      "Батч эмбеддингов для текущих слов имеет размер 3 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  3 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, True, True, False, True]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, True, True, False, True]\n",
      "\n",
      "Неочищенный батч: (None, 10, 12, None, None)\n",
      "Текущий батч индексов слов: tensor([10, 12])\n",
      "Батч эмбеддингов для текущих слов имеет размер 2 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  2 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, True, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, True, True, False, False]\n",
      "\n",
      "Неочищенный батч: (None, None, 8, None, None)\n",
      "Текущий батч индексов слов: tensor([8])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, False, True, False, False]\n",
      "\n",
      "Неочищенный батч: (None, None, 2, None, None)\n",
      "Текущий батч индексов слов: tensor([2])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, False, True, False, False]\n",
      "\n",
      "Неочищенный батч: (None, None, 0, None, None)\n",
      "Текущий батч индексов слов: tensor([0])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [False, False, True, False, False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(179)\n",
    "\n",
    "# Создаем ячейки и инициализируем случайно внутренние состояния ячейки.\n",
    "cell = nn.RNNCell(d_in, d_hidden)\n",
    "h = torch.rand(len(texts_idxs), d_hidden)\n",
    "\n",
    "length_longest_seq = max(map(len, texts_idxs))\n",
    "\n",
    "# Цикл по длине последовательностей\n",
    "for batch_long in zip_longest(*texts_idxs):\n",
    "    print(f\"Неочищенный батч: {batch_long}\")\n",
    "\n",
    "    # Определим последовательности, в которых слова еще не кончились.\n",
    "    mask_leave = [ind is not None for ind in batch_long]\n",
    "\n",
    "    # Почистим батч от None\n",
    "    batch = list(compress(batch_long, mask_leave))\n",
    "\n",
    "    # Переводим батч в тензор\n",
    "    batch = torch.LongTensor(batch)\n",
    "    print(f\"Текущий батч индексов слов: {batch}\")\n",
    "\n",
    "    # Получаем для него эмбеддинги\n",
    "    embeds = embeddings(batch)\n",
    "    print(f\"Батч эмбеддингов для текущих слов имеет размер {shape(embeds)}\")\n",
    "\n",
    "    h_prev = h.clone()\n",
    "\n",
    "    # Подаем RNN ячейке на вход эмбеддинги и\n",
    "    #  соответствующие им внутренние состояния ячейки\n",
    "    h[mask_leave] = cell(embeds, h[mask_leave])\n",
    "    print(f\"Внутренние состояния для последовательностей,\\n\"\n",
    "          f\"  которые пропускались через RNN, имеют размер  {shape(h[mask_leave])}\")\n",
    "    print(f\"Маска последовательностей, которые пока что еще пропускаются\\n\"\n",
    "          f\"  через RNN-ячейку:     {mask_leave}\")\n",
    "    print(f\"Проверка того, для каких последовательностей изменились\\n\"\n",
    "          f\"  внутренние состояния: {(h != h_prev).any(dim=1).tolist()}\")\n",
    "    print()\n",
    "\n",
    "hs.append(h.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь код стал чище.\n",
    "\n",
    "Небольшая проблема такой реализации в том, что на каждой итерации цикла мы просматриваем все предложения и выделяем из них достаточно длинные для текущего шага. Можно сэкономить на этом и на каждой итерации оставлять из еще не выкинутых последовательностей только нужные последовательности.\n",
    "\n",
    "Для этого отсортируем последовательности по длине по убыванию. Тогда на каждом шаге в RNN-ячейку будут подаваться только несколько первых последовательностей. Значит, нам достаточно поддерживать количество еще незакончившихся предложений на каждом шаге. Такое решение будет работать быстрее (анализ сложности приведен в конце главы).\n",
    "\n",
    "Отсортируем тексты в порядке убывания их длины. Для того, чтобы потом можно было восстановить исходный порядок внутренних состояний, сделаем `argsort` для длин последовательностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 4, 0, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_idxs_argsorted = sorted(\n",
    "    range(len(texts_idxs)),\n",
    "    key=lambda i: len(texts_idxs[i]),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "texts_idxs_argsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На `numpy` такое можно реализовать проще с помощью функции `numpy.argsort`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 4, 0, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "texts_idxs_argsorted = np.argsort(list(map(lambda x: -len(x), texts_idxs))).tolist()\n",
    "texts_idxs_argsorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переупорядочим предложения в порядке убывания длины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7, 3, 12, 8, 2, 0], [1, 5, 10], [6, 9], [4], [11]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_idxs_sorted = [texts_idxs[i] for i in texts_idxs_argsorted]\n",
    "texts_idxs_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим обратное отображения для \"отмены\" сортировки для возвращения к первоначальному порядку предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 0, 4, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_idxs_argsort_return = np.argsort(texts_idxs_argsorted).tolist()\n",
    "texts_idxs_argsort_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем ускоренный цикл прохода по предложениям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущий батч индексов слов: tensor([ 7,  1,  6,  4, 11])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([3, 5, 9])\n",
      "Батч эмбеддингов для текущих слов имеет размер 3 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  3 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([12, 10])\n",
      "Батч эмбеддингов для текущих слов имеет размер 2 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  2 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, False, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([8])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, False, False, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([2])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, False, False, False, False]\n",
      "\n",
      "Текущий батч индексов слов: tensor([0])\n",
      "Батч эмбеддингов для текущих слов имеет размер 1 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  1 x 10\n",
      "Маска последовательностей, которые пока что еще пропускаются\n",
      "  через RNN-ячейку:     [False, False, True, False, False]\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, False, False, False, False]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(179)\n",
    "\n",
    "# Создаем ячейки и инициализируем случайно внутренние состояния ячейки.\n",
    "cell = nn.RNNCell(d_in, d_hidden)\n",
    "h = torch.rand(len(texts_idxs), d_hidden)[texts_idxs_argsorted]\n",
    "\n",
    "num_active_seqs = len(texts_idxs)\n",
    "\n",
    "# Цикл по длине последовательностей\n",
    "for step in range(len(texts_idxs_sorted[0])):\n",
    "    # Обновляем счетчик предложений, которые еще не кончились\n",
    "    while len(texts_idxs_sorted[num_active_seqs - 1]) <= step:\n",
    "        num_active_seqs -= 1\n",
    "\n",
    "    # Составляем из них батч для слов\n",
    "    batch = [\n",
    "        text_idxs[step] for text_idxs\n",
    "        in texts_idxs_sorted[:num_active_seqs]\n",
    "    ]\n",
    "\n",
    "    # Переводим батч в тензор\n",
    "    batch = torch.LongTensor(batch)\n",
    "    print(f\"Текущий батч индексов слов: {batch}\")\n",
    "\n",
    "    # Получаем для него эмбеддинги\n",
    "    embeds = embeddings(batch)\n",
    "    print(f\"Батч эмбеддингов для текущих слов имеет размер {shape(embeds)}\")\n",
    "\n",
    "    h_prev = h.clone()\n",
    "\n",
    "    # Подаем RNN ячейке на вход эмбеддинги и\n",
    "    #  соответствующие им внутренние состояния ячейки\n",
    "    h[:num_active_seqs] = cell(embeds, h[:num_active_seqs])\n",
    "\n",
    "    print(f\"Внутренние состояния для последовательностей,\\n\"\n",
    "          f\"  которые пропускались через RNN, имеют размер  {shape(h[:num_active_seqs])}\")\n",
    "    print(f\"Маска последовательностей, которые пока что еще пропускаются\\n\"\n",
    "          f\"  через RNN-ячейку:     {mask_leave}\")\n",
    "    print(f\"Проверка того, для каких последовательностей изменились\\n\"\n",
    "          f\"  внутренние состояния: {(h != h_prev).any(dim=1).tolist()}\")\n",
    "    print()\n",
    "\n",
    "hs.append(h.clone()[texts_idxs_argsort_return])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что все реализации вернули в итоге одинаковые внутренние состояния ячеек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(torch.allclose(h1, h2) for h1 in hs for h2 in hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный метод носит название *упаковки последовательностей*, постольку поскольку для его работы не требуется дополнительная память, и в реализации `torch` все последовательности сливаются в одну, тем самым упаковываясь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Уравнивание длин последовательностей (метод набивки) <a id='sssection_sequences_packing_padding'></a>\n",
    "\n",
    "Другой подход основан на том, что в конец коротких последовательностей добавляются незначимые символы. Для этого вводят специальный символ `<PAD>` (padding — набивка, заполнение), который добавляется в конец последовательностей так, чтобы уравнять их длины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим этот токен в словарь и в отображение слов в индексы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD_TOKEN = \"<PAD>\"\n",
    "vocabulary.add(PAD_TOKEN)\n",
    "word2ind[PAD_TOKEN] = len(word2ind)\n",
    "word2ind[PAD_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обновим слой эмбеддингов. Сохраним в нем старые эмбеддинги для воспроизводимости экспериментов (а точнее для того, чтобы показать, что подходы с упаковкой и с набивкой последовательностей дают разные результаты, но об этом позже):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(14, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_ = nn.Embedding(num_embeddings=len(vocabulary), embedding_dim=d_in)\n",
    "embeddings_.weight.data[:len(vocabulary) - 1] = embeddings.weight.data\n",
    "embeddings = embeddings_\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним все короткие последовательности в конце \"набивкой\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4, 13, 13, 13, 13, 13],\n",
       "        [ 1,  5, 10, 13, 13, 13],\n",
       "        [ 7,  3, 12,  8,  2,  0],\n",
       "        [11, 13, 13, 13, 13, 13],\n",
       "        [ 6,  9, 13, 13, 13, 13]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_longest_seq = max(map(len, texts_idxs))\n",
    "\n",
    "texts_idxs_padded = [\n",
    "    text + [word2ind[PAD_TOKEN]] * (length_longest_seq - len(text))\n",
    "    for text in texts_idxs\n",
    "]\n",
    "torch.LongTensor(texts_idxs_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь эти последовательности одинаковой длины и сложностей с подачей их в RNN-ячейку не возникает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текущий батч индексов слов: tensor([ 4,  1,  7, 11,  6])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([13,  5,  3, 13,  9])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([13, 10, 12, 13, 13])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([13, 13,  8, 13, 13])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([13, 13,  2, 13, 13])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n",
      "Текущий батч индексов слов: tensor([13, 13,  0, 13, 13])\n",
      "Батч эмбеддингов для текущих слов имеет размер 5 x 2\n",
      "Внутренние состояния для последовательностей,\n",
      "  которые пропускались через RNN, имеют размер  5 x 10\n",
      "Проверка того, для каких последовательностей изменились\n",
      "  внутренние состояния: [True, True, True, True, True]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(179)\n",
    "\n",
    "# Создаем ячейки и инициализируем случайно внутренние состояния ячейки.\n",
    "cell = nn.RNNCell(d_in, d_hidden)\n",
    "h = torch.rand(len(texts_idxs), d_hidden)\n",
    "\n",
    "# Цикл по длине последовательностей\n",
    "for batch in zip(*texts_idxs_padded):\n",
    "    # Переводим батч в тензор\n",
    "    batch = torch.LongTensor(batch)\n",
    "    print(f\"Текущий батч индексов слов: {batch}\")\n",
    "\n",
    "    # Получаем для него эмбеддинги\n",
    "    embeds = embeddings(batch)\n",
    "    print(f\"Батч эмбеддингов для текущих слов имеет размер {shape(embeds)}\")\n",
    "\n",
    "    h_prev = h.clone()\n",
    "\n",
    "    # Подаем RNN ячейке на вход эмбеддинги и\n",
    "    #  соответствующие им внутренние состояния ячейки\n",
    "    h = cell(embeds, h)\n",
    "\n",
    "    print(f\"Внутренние состояния для последовательностей,\\n\"\n",
    "          f\"  которые пропускались через RNN, имеют размер  {shape(h)}\")\n",
    "    print(f\"Проверка того, для каких последовательностей изменились\\n\"\n",
    "          f\"  внутренние состояния: {(h != h_prev).any(dim=1).tolist()}\")\n",
    "    print()\n",
    "\n",
    "hs.append(h.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, внутреннее состояние ячейки после пропускания \"набитых\" предложений отличается от состояния ячейки после пропускания упакованных последовательностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hs[0] == hs[-1]).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это связано с тем, что эти способы не эквивалентны друг другу. При набивке предложений модели придется еще дополнительно выучить, что такен `<PAD>` незначимый. На практике с этим проблем обычно не возникает.\n",
    "\n",
    "Также стоит отметить, что второй подход более прожорлив как по памяти, так и по времени исполнения. Первый метод работает за время от суммы длин всех последовательностей $O \\left(\\sum\\limits_{i=1}^{n} l_i \\right)$, где $l_i$ — длина $i$-го предложения, а $n$ — количество предложений, в то время как второй — за $O \\left( n \\cdot \\max\\limits_{1 \\leqslant i \\leqslant n} l_i \\right)$.\n",
    "\n",
    "В случае, когда в батч попало 1 очень длинное предложение и много коротких, первый метод будет работать гораздо быстрее второго.\n",
    "\n",
    "Для того, чтобы метод набивки можно было использовать на практике, обычно перед разбивкой обучающией коллекции на батчи все предложения в обучающей выборке сортируются по длине, после чего батчи формируются из подряд идущих предложений. В таком случае будем иметь батчи со сбалансированными длинами предложений и второй метод будет работать в таком случае быстро. Надо отметить, что в таком случае метод упаковки тоже ускорится, потому что чем меньше итераций будет сделано на питоне и чем больше будет их сделано на том, что лежит под питоном в библиотеке `torch` (C/C++/Cuda) (а сбалансированность длин предложений в одном батче приведет именно к этому), тем быстрее будет работать программа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация подходов упаковки и набивки в `torch` <a id='sssection_sequences_packing_torch'></a>\n",
    "\n",
    "Оба подхода реализованы и в `torch`. С упакованными последовательностями умеют работать только RNN слои. Давайте познакомимся поближе с тем, как пользоваться этими методами в `torch`.\n",
    "\n",
    "#### \"Чистый\" метод упаковки <a id='ssssection_sequences_packing_torch_packing'></a>\n",
    "\n",
    "Для того, чтобы применить метод упаковки, используют функцию `torch.nn.utils.rnn.pack_sequence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 7,  1,  6,  4, 11,  3,  5,  9, 12, 10,  8,  2,  0]), batch_sizes=tensor([5, 3, 2, 1, 1, 1]), sorted_indices=tensor([2, 1, 4, 0, 3]), unsorted_indices=tensor([3, 1, 0, 4, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_sequence, unpack_sequence\n",
    "\n",
    "\n",
    "texts_idxs_packed = pack_sequence(\n",
    "    list(map(torch.LongTensor, texts_idxs)),  # Переводим индексы в torch.Tensor\n",
    "    enforce_sorted=False  # Говорим функции, что последовательности\n",
    "                          #  не отсортированы по длине\n",
    ")\n",
    "\n",
    "texts_idxs_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, эта функция вернула нам объект класса `PackedSequence`. В нем в поле `data` хранятся все переданные индексы, склеенные вместе и в таком порядке, в котором они будут нарезаться на батчи, в поле `batch_sizes` — размеры будущих батчей, в поле `sorted_indices` — argsort набора предложений по их длине по убыванию, в `unsorted_indices` — отображение, отменяющее сортировку. Все эти элементы должны быть уже вам знакомы из [примера выше](#sssection_sequences_packing_filtration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, запакованную последовательность нельзя напрямую подать в слой эмбеддингов. Как это обычно решается — смотрите дальше. Сейчас же для демонстрации инструментов библиотеки `torch` получим сначала эмбеддинги из последовательностей индексов, после чего уже запакуем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-1.1374,  0.7199],\n",
       "        [-0.5868, -0.5840],\n",
       "        [ 1.6448, -0.5534],\n",
       "        [-0.0144,  1.6655],\n",
       "        [-0.5088, -0.5891],\n",
       "        [-0.4719, -0.2825],\n",
       "        [-0.2721, -0.1513],\n",
       "        [-0.5439, -1.2054],\n",
       "        [ 0.1881,  0.6930],\n",
       "        [ 2.3347,  0.5044],\n",
       "        [ 0.6975,  1.7555],\n",
       "        [-0.7170,  0.6318],\n",
       "        [-0.5286,  1.5766]], grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([5, 3, 2, 1, 1, 1]), sorted_indices=tensor([2, 1, 4, 0, 3]), unsorted_indices=tensor([3, 1, 0, 4, 2]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Получаем эмбеддинги по индексам\n",
    "embeds = [\n",
    "    embeddings(torch.LongTensor(text_idxs))\n",
    "    for text_idxs in texts_idxs\n",
    "]\n",
    "\n",
    "embeds_packed = pack_sequence(embeds, enforce_sorted=False)\n",
    "embeds_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-ячейки не умеют обрабатывать запакованные последовательности, зато RNN слои умеют:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-0.1978, -0.1805, -0.1936,  0.1152,  0.2273,  0.1524,  0.0596,  0.1110,\n",
       "          0.1268,  0.4234],\n",
       "        [ 0.0276,  0.1259,  0.0160,  0.1505,  0.1567, -0.1188, -0.1273,  0.3587,\n",
       "          0.0468,  0.4499],\n",
       "        [ 0.2348, -0.2306,  0.5874,  0.3280, -0.3143,  0.2176, -0.4448,  0.4408,\n",
       "          0.5998,  0.4153],\n",
       "        [-0.2152, -0.5696,  0.0992,  0.2150, -0.0444,  0.5213, -0.0448, -0.0092,\n",
       "          0.5531,  0.3797],\n",
       "        [ 0.0358,  0.1153,  0.0392,  0.1569,  0.1404, -0.1087, -0.1398,  0.3626,\n",
       "          0.0682,  0.4489],\n",
       "        [ 0.0892,  0.0223,  0.1362, -0.0101, -0.0137, -0.0842, -0.0705,  0.3328,\n",
       "          0.0660,  0.5279],\n",
       "        [-0.0244, -0.1210, -0.0546,  0.1378, -0.1525,  0.0750, -0.1620,  0.3155,\n",
       "          0.2473,  0.5696],\n",
       "        [ 0.3471,  0.4524,  0.2317,  0.0928,  0.0860, -0.4404,  0.3305,  0.6302,\n",
       "         -0.0901,  0.7319],\n",
       "        [-0.0103, -0.3869,  0.0469,  0.2196, -0.1900,  0.2712, -0.1229,  0.2468,\n",
       "          0.4469,  0.5354],\n",
       "        [ 0.3612, -0.5652,  0.7385,  0.3415, -0.6155,  0.4044, -0.3248,  0.3790,\n",
       "          0.7632,  0.6056],\n",
       "        [ 0.1411, -0.5751,  0.5350,  0.1563, -0.3220,  0.4177,  0.1900,  0.1878,\n",
       "          0.6126,  0.6057],\n",
       "        [ 0.1635,  0.1467,  0.2246, -0.0092,  0.2240, -0.1244,  0.4447,  0.4634,\n",
       "          0.1405,  0.5949],\n",
       "        [-0.3921, -0.4348, -0.3218,  0.1667, -0.0675,  0.4375, -0.0946,  0.0740,\n",
       "          0.5265,  0.4383]], grad_fn=<CatBackward0>), batch_sizes=tensor([5, 3, 2, 1, 1, 1]), sorted_indices=tensor([2, 1, 4, 0, 3]), unsorted_indices=tensor([3, 1, 0, 4, 2]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(d_in, d_hidden)\n",
    "rnn_out, h = rnn(embeds_packed)\n",
    "rnn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы распаковать результат, достаточно применить к нему функцию `nn.utils.rnn.unpack_sequence` — он преобразует `PackedSequence` обратно в список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.2152, -0.5696,  0.0992,  0.2150, -0.0444,  0.5213, -0.0448, -0.0092,\n",
       "           0.5531,  0.3797]], grad_fn=<IndexBackward0>),\n",
       " tensor([[ 0.0276,  0.1259,  0.0160,  0.1505,  0.1567, -0.1188, -0.1273,  0.3587,\n",
       "           0.0468,  0.4499],\n",
       "         [-0.0244, -0.1210, -0.0546,  0.1378, -0.1525,  0.0750, -0.1620,  0.3155,\n",
       "           0.2473,  0.5696],\n",
       "         [ 0.3612, -0.5652,  0.7385,  0.3415, -0.6155,  0.4044, -0.3248,  0.3790,\n",
       "           0.7632,  0.6056]], grad_fn=<IndexBackward0>),\n",
       " tensor([[-0.1978, -0.1805, -0.1936,  0.1152,  0.2273,  0.1524,  0.0596,  0.1110,\n",
       "           0.1268,  0.4234],\n",
       "         [ 0.0892,  0.0223,  0.1362, -0.0101, -0.0137, -0.0842, -0.0705,  0.3328,\n",
       "           0.0660,  0.5279],\n",
       "         [-0.0103, -0.3869,  0.0469,  0.2196, -0.1900,  0.2712, -0.1229,  0.2468,\n",
       "           0.4469,  0.5354],\n",
       "         [ 0.1411, -0.5751,  0.5350,  0.1563, -0.3220,  0.4177,  0.1900,  0.1878,\n",
       "           0.6126,  0.6057],\n",
       "         [ 0.1635,  0.1467,  0.2246, -0.0092,  0.2240, -0.1244,  0.4447,  0.4634,\n",
       "           0.1405,  0.5949],\n",
       "         [-0.3921, -0.4348, -0.3218,  0.1667, -0.0675,  0.4375, -0.0946,  0.0740,\n",
       "           0.5265,  0.4383]], grad_fn=<IndexBackward0>),\n",
       " tensor([[ 0.0358,  0.1153,  0.0392,  0.1569,  0.1404, -0.1087, -0.1398,  0.3626,\n",
       "           0.0682,  0.4489]], grad_fn=<IndexBackward0>),\n",
       " tensor([[ 0.2348, -0.2306,  0.5874,  0.3280, -0.3143,  0.2176, -0.4448,  0.4408,\n",
       "           0.5998,  0.4153],\n",
       "         [ 0.3471,  0.4524,  0.2317,  0.0928,  0.0860, -0.4404,  0.3305,  0.6302,\n",
       "          -0.0901,  0.7319]], grad_fn=<IndexBackward0>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_sequence(rnn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Чистый\" метод набивки <a id='ssssection_sequences_packing_torch_padding'></a>\n",
    "\n",
    "Для того, чтобы применить метод набивки, используют функцию `nn.utils.rnn.pad_sequence`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  1,  7, 11,  6],\n",
       "        [13,  5,  3, 13,  9],\n",
       "        [13, 10, 12, 13, 13],\n",
       "        [13, 13,  8, 13, 13],\n",
       "        [13, 13,  2, 13, 13],\n",
       "        [13, 13,  0, 13, 13]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, unpad_sequence\n",
    "\n",
    "\n",
    "texts_idxs_padded = pad_sequence(\n",
    "    map(torch.LongTensor, texts_idxs),\n",
    "    padding_value=word2ind[PAD_TOKEN]\n",
    ")\n",
    "texts_idxs_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аргумент `padding_value` в этой функции отвечает за то, каким значением набивать последовательности. Обратите внимание, что возвращенный тензор имеет размерность $\\text{max_len} \\times \\text{batch_size}$.\n",
    "\n",
    "Далее такой тензор можно сразу подавать в слой эмбеддингов, а затем и в RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер батча эмбеддингов: 6 x 5 x 2\n",
      "Размер выхода RNN: 6 x 5 x 10\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.RNN(d_in, d_hidden)\n",
    "\n",
    "embeds_padded = embeddings(torch.LongTensor(texts_idxs_padded))\n",
    "print(f\"Размер батча эмбеддингов: {shape(embeds_padded)}\")\n",
    "\n",
    "rnn_out, h = rnn(embeds_padded)\n",
    "print(f\"Размер выхода RNN: {shape(rnn_out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для отбрасывания лишних элементов в `rnn_out` можно воспользоваться функцией `unpad_sequence` (необходимо вторым аргументом ей передать длины всех предложений):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.7152, -0.2390,  0.0960,  0.0675, -0.1831,  0.3455,  0.1101,  0.0430,\n",
       "          -0.1950, -0.2728]], grad_fn=<IndexBackward0>),\n",
       " tensor([[-0.2731,  0.4285, -0.2558, -0.5957, -0.0011,  0.2304,  0.4169, -0.1561,\n",
       "           0.2276,  0.1888],\n",
       "         [-0.0169,  0.1087, -0.0405, -0.1913, -0.0772,  0.0100,  0.0596, -0.1091,\n",
       "           0.3642,  0.1885],\n",
       "         [-0.1324, -0.6316,  0.4477,  0.5264, -0.2030, -0.5681, -0.1694,  0.3114,\n",
       "           0.1090, -0.6532]], grad_fn=<IndexBackward0>),\n",
       " tensor([[-0.6384,  0.3147, -0.2087, -0.4661, -0.0902,  0.5102,  0.3545, -0.1971,\n",
       "           0.0114,  0.1542],\n",
       "         [-0.0067,  0.2284, -0.1465, -0.1958,  0.0088,  0.1258,  0.1312, -0.0881,\n",
       "           0.3318,  0.3768],\n",
       "         [-0.3454, -0.1573,  0.0601, -0.0046, -0.0983,  0.0285,  0.0690, -0.0775,\n",
       "           0.1750, -0.2279],\n",
       "         [-0.7222, -0.4690,  0.3725,  0.4302, -0.2899,  0.0966, -0.0270,  0.2580,\n",
       "          -0.1637, -0.3741],\n",
       "         [-0.7802,  0.4029, -0.1321, -0.3001, -0.1191,  0.4891,  0.4793,  0.0907,\n",
       "          -0.1734,  0.3664],\n",
       "         [-0.5541, -0.0450, -0.0905,  0.1662, -0.0388,  0.4476,  0.0096, -0.0058,\n",
       "          -0.0923,  0.2875]], grad_fn=<IndexBackward0>),\n",
       " tensor([[-0.2620,  0.4095, -0.2432, -0.5818, -0.0022,  0.2083,  0.4086, -0.1427,\n",
       "           0.2261,  0.1692]], grad_fn=<IndexBackward0>),\n",
       " tensor([[ 0.0068, -0.2385,  0.1397, -0.0318, -0.0473, -0.3866,  0.1364,  0.2399,\n",
       "           0.1528, -0.3914],\n",
       "         [-0.3330,  0.5737, -0.1592, -0.6988, -0.1245,  0.0814,  0.5155, -0.0604,\n",
       "           0.2877,  0.3181]], grad_fn=<IndexBackward0>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpad_sequence(rnn_out, list(map(len, texts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Совмещение обоих подходов <a id='ssssection_sequences_packing_torch_mixed'></a>\n",
    "\n",
    "Для удобства в `torch` можно переходить от одного представления батча последовательностей к другому с помощью специальных функций `pack_padded_sequence` и `pad_packed_sequence`.\n",
    "\n",
    "У нас уже есть подсчитанные представления `texts_idxs_padded` и `texts_idxs_packed`. Попробуем перевести их друг в друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`texts_idxs_padded` $\\rightarrow$ `texts_idxs_packed`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 7,  1,  6,  4, 11,  3,  5,  9, 12, 10,  8,  2,  0]), batch_sizes=tensor([5, 3, 2, 1, 1, 1]), sorted_indices=tensor([2, 1, 4, 0, 3]), unsorted_indices=tensor([3, 1, 0, 4, 2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "pack_padded_sequence(\n",
    "    texts_idxs_padded,\n",
    "    lengths=list(map(len, texts)),\n",
    "    enforce_sorted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([ 7,  1,  6,  4, 11,  3,  5,  9, 12, 10,  8,  2,  0]), batch_sizes=tensor([5, 3, 2, 1, 1, 1]), sorted_indices=tensor([2, 1, 4, 0, 3]), unsorted_indices=tensor([3, 1, 0, 4, 2]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_idxs_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что у функции `pack_padded_sequence` имеется обязательный аргумент `lengths`, с помощью которого определяются длины последовательностей в батче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`texts_idxs_packed` $\\rightarrow$ `texts_idxs_padded`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  1,  7, 11,  6],\n",
       "         [13,  5,  3, 13,  9],\n",
       "         [13, 10, 12, 13, 13],\n",
       "         [13, 13,  8, 13, 13],\n",
       "         [13, 13,  2, 13, 13],\n",
       "         [13, 13,  0, 13, 13]]),\n",
       " tensor([1, 3, 6, 1, 2]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_packed_sequence(\n",
    "    texts_idxs_packed,\n",
    "    padding_value=word2ind[PAD_TOKEN]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  1,  7, 11,  6],\n",
       "        [13,  5,  3, 13,  9],\n",
       "        [13, 10, 12, 13, 13],\n",
       "        [13, 13,  8, 13, 13],\n",
       "        [13, 13,  2, 13, 13],\n",
       "        [13, 13,  0, 13, 13]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_idxs_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `pad_packed_sequence` возвращает помимо \"набитых\" последовательной еще и их длины."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что при упаковке последовательностей у нас возникли проблемы с их подачей в слой эмбеддингов. В качестве решения тут можно предложить следующий подход — сначала мы индексы \"набьем\" до одинаковой длины, затем получившийся тензор запакуем, и его пропустим через RNN. Посмотрим на реализацию такого смешанного подхода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.4581,  0.0114, -0.0217, -0.4463, -0.4853,  0.3156,  0.0222,  0.3450,\n",
       "         -0.4610,  0.1161],\n",
       "        [ 0.3737,  0.0686,  0.3053, -0.2079, -0.2685,  0.1604,  0.0291,  0.0204,\n",
       "         -0.3313,  0.4571],\n",
       "        [-0.2081,  0.2748,  0.7509,  0.3890,  0.3132,  0.1711, -0.2502, -0.3613,\n",
       "          0.2424,  0.4559],\n",
       "        [ 0.1565,  0.1149,  0.1845, -0.2443, -0.2950,  0.4228, -0.1740,  0.3185,\n",
       "         -0.2040, -0.1505],\n",
       "        [ 0.3556,  0.0760,  0.3268, -0.1865, -0.2485,  0.1601,  0.0195,  0.0053,\n",
       "         -0.3127,  0.4584],\n",
       "        [ 0.0881, -0.0575,  0.5058, -0.2277, -0.0756,  0.1411,  0.2543, -0.4965,\n",
       "         -0.1153,  0.5111],\n",
       "        [ 0.2245,  0.0670,  0.3176,  0.0666, -0.2472,  0.0316,  0.1115, -0.2680,\n",
       "         -0.1513,  0.4690],\n",
       "        [ 0.6247,  0.0740,  0.0403,  0.3284, -0.4781, -0.1878, -0.0627,  0.3059,\n",
       "         -0.5863,  0.5624],\n",
       "        [ 0.2088,  0.1689,  0.0619,  0.1565, -0.3061,  0.0726, -0.1028,  0.0952,\n",
       "         -0.2498,  0.1488],\n",
       "        [-0.3634,  0.4107,  0.7304,  0.6550,  0.2901,  0.1424, -0.3624, -0.3613,\n",
       "          0.4578,  0.2904],\n",
       "        [-0.0510,  0.2100,  0.4581,  0.0820, -0.1476,  0.4106, -0.1981,  0.0499,\n",
       "          0.1015, -0.0058],\n",
       "        [ 0.3852, -0.0695,  0.1102, -0.2103, -0.3433,  0.2512, -0.0343,  0.3052,\n",
       "         -0.5480,  0.1745],\n",
       "        [ 0.1232,  0.0119,  0.2510, -0.3021, -0.3467,  0.3882,  0.1294, -0.0395,\n",
       "         -0.1782,  0.0356]], grad_fn=<CatBackward0>), batch_sizes=tensor([5, 3, 2, 1, 1, 1]), sorted_indices=tensor([2, 1, 4, 0, 3]), unsorted_indices=tensor([3, 1, 0, 4, 2]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Набивка индексов\n",
    "texts_idxs_padded = pad_sequence(\n",
    "    map(torch.LongTensor, texts_idxs),\n",
    "    padding_value=word2ind[PAD_TOKEN]\n",
    ")\n",
    "\n",
    "# Получение эмбеддингов\n",
    "embeds_padded = embeddings(texts_idxs_padded)\n",
    "\n",
    "# Упаковка эмбеддингов\n",
    "embeds_packed = pack_padded_sequence(\n",
    "    embeds_padded,\n",
    "    lengths=list(map(len, texts)),\n",
    "    enforce_sorted=False\n",
    ")\n",
    "\n",
    "# Пропуск эмбеддингов через RNN\n",
    "rnn = torch.nn.RNN(d_in, d_hidden)\n",
    "\n",
    "rnn_out, h = rnn(embeds_packed)\n",
    "rnn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что в силу своей природы RNN обрабатывает приходящие последовательности последовательно (в цикле), из-за чего оптимизация с помощью упаковки последовательности работает. [Трансформер](https://arxiv.org/abs/1706.03762) же в свою очередь обрабатывает одновременно всю последовательность целиком (из-за слоев self-attention), поэтому реализация метода упаковки для трансформера имеет свои сложности, и в `torch` этот подход не реализован. При работе с трансформером приходится работать с помощью метода набивки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Работа загрузчика данных в `torch` с текстами\n",
    "\n",
    "Для работы с данными в `torch` используют две сущности — набор данных (data set) и загрузчик данных (data loader). При работе с набором текстов в загрузчике данных стоит указывать `collate_fn` для склейки нескольких предложений в один батч. Для демонстрации этого давайте сначал сконструируем игрушечный набор данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from random import randint, choices\n",
    "from string import ascii_lowercase\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"Набор данных из n случайных предложений\"\"\"\n",
    "    def __init__(self, *, n=10, vocab_size):\n",
    "        \"\"\"\n",
    "        n — число генерируемых предложений,\n",
    "        vocab_size — размер словаря.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n = n\n",
    "        random.seed(n)  # Для воспроизводимости экспериментов\n",
    "        self.texts = [\n",
    "            torch.LongTensor([\n",
    "                # Вместо слова генерируем его индекс как\n",
    "                #  будто бы мы уже перевели все слова в индексы\n",
    "                randint(0, vocab_size - 1)\n",
    "                for _ in range(randint(1, 8))\n",
    "            ])\n",
    "            for _ in range(n)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        return self.texts[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что лежит в датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6])\n",
      "tensor([ 9,  0,  3,  7, 13,  7, 13,  4])\n",
      "tensor([0, 8, 7])\n",
      "tensor([ 1,  3, 11,  5,  0,  6])\n",
      "tensor([9, 5, 6])\n",
      "tensor([ 4, 13, 10,  4,  7,  2, 10])\n",
      "tensor([10,  5,  2,  7, 12])\n",
      "tensor([7, 9, 6, 0])\n",
      "tensor([3])\n",
      "tensor([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(vocab_size=len(vocabulary))\n",
    "for x in dataset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы теперь попробуем создать загрузчик данных от такого набора данных и начнем его использовать, то код упадет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught error: stack expects each tensor to be equal size, but got [1] at entry 0 and [8] at entry 1\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=3)\n",
    "\n",
    "try:\n",
    "    batch = next(iter(dataloader))\n",
    "except RuntimeError as e:\n",
    "    print(f\"Caught error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка красноречива — предложения в батче оказались разной длины и загрузчик данных не знает, что с ними делать. Проблема в том, что для создания батча из отдельных его элементов `DataLoader` вызывает функцию `collate_fn` (collate — собирать, объединять). По умолчанию она просто склеивает поданные ей тензоры в один, поэтому тензоры разных размеров и вызывают ошибку. Решение тут простое — нужно задать свою функцию `collate_fn`.\n",
    "\n",
    "Мы уже прошли два подхода для работы с предложениями разной длины, тогда и напишем две разных `collate_fn` функции:\n",
    "\n",
    "* С запаковкой предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PackedSequence(data=tensor([ 9,  0,  6,  0,  8,  3,  7,  7, 13,  7, 13,  4]), batch_sizes=tensor([3, 2, 2, 1, 1, 1, 1, 1]), sorted_indices=tensor([1, 2, 0]), unsorted_indices=tensor([2, 0, 1]))\n",
      "\n",
      "PackedSequence(data=tensor([ 4,  1,  9, 13,  3,  5, 10, 11,  6,  4,  5,  7,  0,  2,  6, 10]), batch_sizes=tensor([3, 3, 3, 2, 2, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))\n",
      "\n",
      "PackedSequence(data=tensor([10,  7,  3,  5,  9,  2,  6,  7,  0, 12]), batch_sizes=tensor([3, 2, 2, 2, 1]), sorted_indices=tensor([0, 1, 2]), unsorted_indices=tensor([0, 1, 2]))\n",
      "\n",
      "PackedSequence(data=tensor([3, 4, 8]), batch_sizes=tensor([1, 1, 1]), sorted_indices=tensor([0]), unsorted_indices=tensor([0]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collate_pack(batch):\n",
    "    \"\"\"batch — список тензоров\"\"\"\n",
    "    return pack_sequence(batch, enforce_sorted=False)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=3, collate_fn=collate_pack)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* И с набивкой предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6, 13, 13, 13, 13, 13, 13, 13],\n",
      "        [ 9,  0,  3,  7, 13,  7, 13,  4],\n",
      "        [ 0,  8,  7, 13, 13, 13, 13, 13]])\n",
      "\n",
      "tensor([[ 1,  3, 11,  5,  0,  6, 13],\n",
      "        [ 9,  5,  6, 13, 13, 13, 13],\n",
      "        [ 4, 13, 10,  4,  7,  2, 10]])\n",
      "\n",
      "tensor([[10,  5,  2,  7, 12],\n",
      "        [ 7,  9,  6,  0, 13],\n",
      "        [ 3, 13, 13, 13, 13]])\n",
      "\n",
      "tensor([[3, 4, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def collate_pad(batch):\n",
    "    \"\"\"batch — список тензоров\"\"\"\n",
    "    return pad_sequence(batch, batch_first=True,\n",
    "                        padding_value=word2ind[PAD_TOKEN])\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=3, collate_fn=collate_pad)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюме главы**\n",
    "1. Существует два похода для работы с батчом последовательностей разной длины: метод упаковки и метод набивки. Оба подхода реализованы в `torch`.\n",
    "2. Метод упаковки позволяет делать меньше вычислений при работе с RNN.\n",
    "3. Предварительная сортировка выборки по убыванию длин последовательностей поможет еще сильнее повысить эффективность работы RNN.\n",
    "4. Упаковка последовательностей в `torch` реализована только для RNN.\n",
    "5. При работе с текстами в загрузчиках данных стоит указывать `collate_fn` для правильного образования батчей.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Специальные токены <a id='ssection_special_tokens'></a>\n",
    "\n",
    "В прошлых главах у нас зашла речь о специальных токенах. Мы уже успели познакомиться с токенами `<UNK>` и `<PAD>`. Оказывается, в NLP есть и другие часто используемые спецтокены:\n",
    "\n",
    "* `<BOS>`, begin of sequence — начало последовательности. Используется при генерации текстов в качестве начального токена, с которого начинается генерация.\n",
    "* `<EOS>`, end of sequence — конец предложения. Используется также при генерации текстов для того, чтобы модель могла нам сообщить о том, что она закончила генерацию.\n",
    "\n",
    "![BOS and EOS tokens](images/bos-eos-tokens.pbm \"BOS and EOS tokens\")\n",
    "\n",
    "* `<MASK>` — используется в BERT-подобных моделях для маскирования токенов в тексте в задачи MLM — Masked-Language Modeling. Суть задачи заключается в том, чтобы модель научилась предсказывать замаскированные слова по их контексту.\n",
    "* `<CLS>` — используется в BERT-подобных моделях для решения задачи классификации всего предложения и заменяет собой пулинг эмбеддингов токенов предложения.\n",
    "\n",
    "![MASK and CLS tokens](images/mask-cls-tokens.png \"MASK and CLS tokens\")\n",
    "\n",
    "* `<SEP>` — используется в BERT-подобных моделях для разделения двух предложений для задачи NSP — Next Sentence Prediction. Суть задачи заключается в том, чтобы модель научилась  определять, идут ли два предложения в тексте друг за другом или нет.\n",
    "\n",
    "![SEP token](images/sep-token.png \"SEP token\")\n",
    "\n",
    "Специальные токены могут иметь разный вид и не обязательно записываются в треугольных скобках. Например, спецтокены могут записывать в квадратных скобках, но сути дела это не меняет.\n",
    "\n",
    "**Резюме главы**\n",
    "1. При обработке естественного языка часто используют специальные токены, которые позволяют решать определенные задачи.\n",
    "\n",
    "[К оглавлению](#toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
